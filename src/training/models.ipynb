{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from itertools import product\n",
    "\n",
    "import IPython.display as ipd\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.api.callbacks import EarlyStopping\n",
    "from keras.api.layers import LSTM, Conv2D, Dense, Dropout, Flatten, Input, MaxPooling2D\n",
    "from keras.api.models import Sequential, load_model\n",
    "from keras.api.regularizers import l2\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, MinMaxScaler\n",
    "\n",
    "# Directories\n",
    "DATA_DIR = \"../../data\"\n",
    "AUDIOS_DIR = DATA_DIR + \"/audios\"\n",
    "POS_AUDIOS_DIR = AUDIOS_DIR + \"/positive\"\n",
    "NEG_AUDIOS_DIR = AUDIOS_DIR + \"/negative\"\n",
    "\n",
    "MODELS_DIR = \"../../models\"\n",
    "PLOTS_DIR = \"../../plots\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mayor constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"ff\"  # 'cnn', 'lstm', 'ff'\n",
    "FEATURE = \"lfcc\"  # 'mfcc', 'chroma', 'lfcc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (10000, 12, 313)\n",
      "y shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "with open(f\"{DATA_DIR}/{FEATURE}.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Positive data\n",
    "positive_data = np.array(data[\"positive\"])\n",
    "positive_labels = np.ones(positive_data.shape[0])\n",
    "pnames = data[\"pnames\"]\n",
    "\n",
    "# Negative data\n",
    "negative_data = np.array(data[\"negative\"])\n",
    "negative_labels = np.zeros(negative_data.shape[0])\n",
    "nnames = data[\"nnames\"]\n",
    "\n",
    "# Concatenate the data\n",
    "X = np.concatenate((positive_data, negative_data), axis=0)\n",
    "y = np.concatenate((positive_labels, negative_labels), axis=0)\n",
    "names = np.concatenate((pnames, nnames), axis=0)\n",
    "\n",
    "# Establecer la semilla aleatoria\n",
    "seed = 1  # Elige cualquier número entero como semilla\n",
    "rng = np.random.default_rng(seed)\n",
    "\n",
    "# Shuffle the data using the random number generator\n",
    "idx = rng.permutation(len(X))\n",
    "\n",
    "X = X[idx]\n",
    "y = y[idx]\n",
    "names = names[idx]\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\angel\\anaconda3\\envs\\TFG\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 10 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step\n",
      "(10000, 78, 12, 1)\n",
      "[[[[0.83060974]\n",
      "   [0.28658724]\n",
      "   [0.59740955]\n",
      "   ...\n",
      "   [0.40920192]\n",
      "   [0.56993747]\n",
      "   [0.4996098 ]]\n",
      "\n",
      "  [[0.84445775]\n",
      "   [0.17025794]\n",
      "   [0.6580201 ]\n",
      "   ...\n",
      "   [0.39519733]\n",
      "   [0.612451  ]\n",
      "   [0.4178421 ]]\n",
      "\n",
      "  [[0.84018874]\n",
      "   [0.19158062]\n",
      "   [0.60329574]\n",
      "   ...\n",
      "   [0.40150952]\n",
      "   [0.63888186]\n",
      "   [0.3920681 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.800269  ]\n",
      "   [0.16760196]\n",
      "   [0.76788884]\n",
      "   ...\n",
      "   [0.37595394]\n",
      "   [0.6050345 ]\n",
      "   [0.4806747 ]]\n",
      "\n",
      "  [[0.78699505]\n",
      "   [0.21286005]\n",
      "   [0.72147405]\n",
      "   ...\n",
      "   [0.3155495 ]\n",
      "   [0.63188136]\n",
      "   [0.42771748]]\n",
      "\n",
      "  [[0.81202483]\n",
      "   [0.3372992 ]\n",
      "   [0.6768875 ]\n",
      "   ...\n",
      "   [0.4934861 ]\n",
      "   [0.65454   ]\n",
      "   [0.51404107]]]\n",
      "\n",
      "\n",
      " [[[0.672771  ]\n",
      "   [0.42719722]\n",
      "   [0.4659889 ]\n",
      "   ...\n",
      "   [0.4588887 ]\n",
      "   [0.5727831 ]\n",
      "   [0.4673935 ]]\n",
      "\n",
      "  [[0.6328412 ]\n",
      "   [0.46765032]\n",
      "   [0.4658005 ]\n",
      "   ...\n",
      "   [0.46982434]\n",
      "   [0.5966268 ]\n",
      "   [0.40538374]]\n",
      "\n",
      "  [[0.6242092 ]\n",
      "   [0.46751374]\n",
      "   [0.4712818 ]\n",
      "   ...\n",
      "   [0.4385921 ]\n",
      "   [0.62749743]\n",
      "   [0.39302787]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.61644226]\n",
      "   [0.50435615]\n",
      "   [0.46216103]\n",
      "   ...\n",
      "   [0.4775135 ]\n",
      "   [0.57557636]\n",
      "   [0.4315666 ]]\n",
      "\n",
      "  [[0.6334541 ]\n",
      "   [0.47997057]\n",
      "   [0.47288403]\n",
      "   ...\n",
      "   [0.38957086]\n",
      "   [0.60479665]\n",
      "   [0.3994923 ]]\n",
      "\n",
      "  [[0.63817245]\n",
      "   [0.63176775]\n",
      "   [0.57146204]\n",
      "   ...\n",
      "   [0.56682575]\n",
      "   [0.60999805]\n",
      "   [0.515548  ]]]\n",
      "\n",
      "\n",
      " [[[0.6524949 ]\n",
      "   [0.4282967 ]\n",
      "   [0.47604927]\n",
      "   ...\n",
      "   [0.45660454]\n",
      "   [0.5409129 ]\n",
      "   [0.51114833]]\n",
      "\n",
      "  [[0.6265796 ]\n",
      "   [0.5061048 ]\n",
      "   [0.46175587]\n",
      "   ...\n",
      "   [0.45259544]\n",
      "   [0.59648305]\n",
      "   [0.44699532]]\n",
      "\n",
      "  [[0.5933716 ]\n",
      "   [0.5140226 ]\n",
      "   [0.46735957]\n",
      "   ...\n",
      "   [0.44169545]\n",
      "   [0.6078997 ]\n",
      "   [0.4392223 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.56518865]\n",
      "   [0.5730413 ]\n",
      "   [0.42225686]\n",
      "   ...\n",
      "   [0.42080092]\n",
      "   [0.60644007]\n",
      "   [0.4542431 ]]\n",
      "\n",
      "  [[0.5785062 ]\n",
      "   [0.56495166]\n",
      "   [0.4311708 ]\n",
      "   ...\n",
      "   [0.35105577]\n",
      "   [0.6294376 ]\n",
      "   [0.40854153]]\n",
      "\n",
      "  [[0.6056417 ]\n",
      "   [0.68096274]\n",
      "   [0.54333806]\n",
      "   ...\n",
      "   [0.5255083 ]\n",
      "   [0.6334165 ]\n",
      "   [0.525583  ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.5644606 ]\n",
      "   [0.61535966]\n",
      "   [0.36300048]\n",
      "   ...\n",
      "   [0.40679753]\n",
      "   [0.5880575 ]\n",
      "   [0.479209  ]]\n",
      "\n",
      "  [[0.5107523 ]\n",
      "   [0.63153476]\n",
      "   [0.34631452]\n",
      "   ...\n",
      "   [0.42063308]\n",
      "   [0.6175159 ]\n",
      "   [0.40689033]]\n",
      "\n",
      "  [[0.54168904]\n",
      "   [0.597627  ]\n",
      "   [0.3758629 ]\n",
      "   ...\n",
      "   [0.42941517]\n",
      "   [0.6172924 ]\n",
      "   [0.41631785]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.67314124]\n",
      "   [0.49755424]\n",
      "   [0.3226438 ]\n",
      "   ...\n",
      "   [0.46260703]\n",
      "   [0.5353785 ]\n",
      "   [0.5178859 ]]\n",
      "\n",
      "  [[0.63865495]\n",
      "   [0.5438683 ]\n",
      "   [0.3498792 ]\n",
      "   ...\n",
      "   [0.34747708]\n",
      "   [0.57631814]\n",
      "   [0.45736393]]\n",
      "\n",
      "  [[0.6234012 ]\n",
      "   [0.6926168 ]\n",
      "   [0.45937732]\n",
      "   ...\n",
      "   [0.5157367 ]\n",
      "   [0.6250574 ]\n",
      "   [0.5361046 ]]]\n",
      "\n",
      "\n",
      " [[[0.5954693 ]\n",
      "   [0.5653912 ]\n",
      "   [0.406931  ]\n",
      "   ...\n",
      "   [0.4270031 ]\n",
      "   [0.52770895]\n",
      "   [0.5045739 ]]\n",
      "\n",
      "  [[0.5404214 ]\n",
      "   [0.64136213]\n",
      "   [0.33506393]\n",
      "   ...\n",
      "   [0.41160667]\n",
      "   [0.57673424]\n",
      "   [0.44041768]]\n",
      "\n",
      "  [[0.54975   ]\n",
      "   [0.6019362 ]\n",
      "   [0.3628907 ]\n",
      "   ...\n",
      "   [0.38533148]\n",
      "   [0.6017694 ]\n",
      "   [0.42338994]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.563619  ]\n",
      "   [0.6457595 ]\n",
      "   [0.30835497]\n",
      "   ...\n",
      "   [0.43841255]\n",
      "   [0.5629304 ]\n",
      "   [0.44142517]]\n",
      "\n",
      "  [[0.5619934 ]\n",
      "   [0.6495769 ]\n",
      "   [0.2965048 ]\n",
      "   ...\n",
      "   [0.4008499 ]\n",
      "   [0.59656024]\n",
      "   [0.40022907]]\n",
      "\n",
      "  [[0.5918129 ]\n",
      "   [0.71847993]\n",
      "   [0.44793072]\n",
      "   ...\n",
      "   [0.54786503]\n",
      "   [0.61587656]\n",
      "   [0.530998  ]]]\n",
      "\n",
      "\n",
      " [[[0.55906844]\n",
      "   [0.5656908 ]\n",
      "   [0.45823935]\n",
      "   ...\n",
      "   [0.49437445]\n",
      "   [0.47258613]\n",
      "   [0.5301146 ]]\n",
      "\n",
      "  [[0.51723206]\n",
      "   [0.5870195 ]\n",
      "   [0.47558016]\n",
      "   ...\n",
      "   [0.5349601 ]\n",
      "   [0.478111  ]\n",
      "   [0.5077002 ]]\n",
      "\n",
      "  [[0.50799173]\n",
      "   [0.59083176]\n",
      "   [0.48106623]\n",
      "   ...\n",
      "   [0.5115146 ]\n",
      "   [0.52294636]\n",
      "   [0.4863079 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.5328096 ]\n",
      "   [0.57587785]\n",
      "   [0.50806385]\n",
      "   ...\n",
      "   [0.48814586]\n",
      "   [0.5012064 ]\n",
      "   [0.55428153]]\n",
      "\n",
      "  [[0.5498238 ]\n",
      "   [0.54192406]\n",
      "   [0.5172649 ]\n",
      "   ...\n",
      "   [0.40702826]\n",
      "   [0.54522145]\n",
      "   [0.5016273 ]]\n",
      "\n",
      "  [[0.5817048 ]\n",
      "   [0.64256924]\n",
      "   [0.6244453 ]\n",
      "   ...\n",
      "   [0.5513944 ]\n",
      "   [0.58088815]\n",
      "   [0.57763803]]]]\n"
     ]
    }
   ],
   "source": [
    "# Funciones auxiliares\n",
    "def normalize(X):\n",
    "    return (X - np.mean(X)) / np.std(X)\n",
    "\n",
    "\n",
    "def sub_extra_column(X):\n",
    "    return X[:, :, :-1]\n",
    "\n",
    "\n",
    "def transpose(X):\n",
    "    return X.transpose(0, 2, 1)\n",
    "\n",
    "\n",
    "# Definición del pipeline\n",
    "preprocessing_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"normalize\", FunctionTransformer(normalize, validate=False)),  # Normalización\n",
    "        (\"scale\", MinMaxScaler(feature_range=(0, 1))),  # Escalado\n",
    "        (\n",
    "            \"sub_column\",\n",
    "            FunctionTransformer(sub_extra_column, validate=False),\n",
    "        ),  # Quitar columna\n",
    "        (\"transpose\", FunctionTransformer(transpose, validate=False)),  # Transponer\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Aplicar el pipeline\n",
    "def process_and_encode(X):\n",
    "    # Reshape para MinMaxScaler (flatten y restaurar)\n",
    "    X = preprocessing_pipeline.named_steps[\"normalize\"].transform(X)\n",
    "    X = (\n",
    "        preprocessing_pipeline.named_steps[\"scale\"]\n",
    "        .fit_transform(X.reshape(X.shape[0], -1))\n",
    "        .reshape(X.shape)\n",
    "    )\n",
    "    X = preprocessing_pipeline.named_steps[\"sub_column\"].transform(X)\n",
    "    X = preprocessing_pipeline.named_steps[\"transpose\"].transform(X)\n",
    "\n",
    "    # Predicción con el encoder\n",
    "    encoder = load_model(f\"{MODELS_DIR}/encoders/encoder_{FEATURE}.keras\")\n",
    "    X = X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)\n",
    "    XC = encoder.predict(X)\n",
    "    return XC\n",
    "\n",
    "\n",
    "X = process_and_encode(X)\n",
    "print(X.shape)\n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 78, 12, 1)\n",
      "(10000,)\n",
      "(8000, 78, 12, 1)\n",
      "(8000,)\n",
      "(2000, 78, 12, 1)\n",
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "percent = 1\n",
    "trn_size = 0.8\n",
    "tst_size = 1 - trn_size\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "X_train, X_tst, y_train, y_tst, idxtrn, idxtst = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    np.arange(len(X)),\n",
    "    train_size=trn_size * percent,\n",
    "    test_size=tst_size * percent,\n",
    "    stratify=y,\n",
    "    random_state=seed,\n",
    ")\n",
    "\n",
    "# Mantain the names of the audios\n",
    "names = np.array(names)\n",
    "namestrn = names[idxtrn]\n",
    "namestst = names[idxtst]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_tst.shape)\n",
    "print(y_tst.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"FF\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"FF\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">936</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">119,936</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten_4 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m936\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_18 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m119,936\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_22 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_19 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_23 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_20 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_24 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_21 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">130,305</span> (509.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m130,305\u001b[0m (509.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">130,305</span> (509.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m130,305\u001b[0m (509.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def FF(optimizer=\"adam\", units=128, activation=\"relu\", dropout_rate=0.2):\n",
    "    model = Sequential(\n",
    "        [\n",
    "            Input(shape=(X.shape[1], X.shape[2], 1)),\n",
    "            Flatten(),\n",
    "            Dense(units, activation=activation),\n",
    "            Dropout(dropout_rate),\n",
    "            Dense(units // 2, activation=activation),\n",
    "            Dropout(dropout_rate),\n",
    "            Dense(units // 4, activation=activation),\n",
    "            Dropout(dropout_rate),\n",
    "            Dense(1, activation=\"sigmoid\"),\n",
    "        ],\n",
    "        name=\"FF\",\n",
    "    )\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "FF().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"CNN\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"CNN\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">78</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">39</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">39</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">39</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">808</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,216</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,936</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m78\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m4\u001b[0m)      │           \u001b[38;5;34m200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_8 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m39\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m4\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_25 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m39\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m4\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_9 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m39\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m8\u001b[0m)      │           \u001b[38;5;34m808\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_9 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m8\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_26 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m8\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_10 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │         \u001b[38;5;34m3,216\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_10 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m16\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_27 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m16\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_11 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m4,640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_11 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_28 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_5 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m288\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m)             │         \u001b[38;5;34m6,936\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_29 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m)             │           \u001b[38;5;34m300\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_30 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_24 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m13\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">16,113</span> (62.94 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m16,113\u001b[0m (62.94 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">16,113</span> (62.94 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m16,113\u001b[0m (62.94 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def CNN(optimizer=\"adam\", units=24, activation=\"relu\", dropout_rate=0.2):\n",
    "    model = Sequential(\n",
    "        [\n",
    "            Input(shape=(X.shape[1], X.shape[2], X.shape[3])),\n",
    "            Conv2D(4, (7, 7), activation=activation, padding=\"same\"),\n",
    "            MaxPooling2D((2, 1)),\n",
    "            Dropout(dropout_rate),\n",
    "            Conv2D(8, (5, 5), activation=activation, padding=\"same\"),\n",
    "            MaxPooling2D((3, 1)),\n",
    "            Dropout(dropout_rate),\n",
    "            Conv2D(16, (5, 5), activation=activation, padding=\"same\"),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            Dropout(dropout_rate),\n",
    "            Conv2D(32, (3, 3), activation=activation, padding=\"same\"),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            Dropout(dropout_rate),\n",
    "            Flatten(),\n",
    "            Dense(units, activation=activation),\n",
    "            Dropout(dropout_rate),\n",
    "            Dense(units // 2, activation=activation),\n",
    "            Dropout(dropout_rate),\n",
    "            Dense(1, activation=\"sigmoid\"),\n",
    "        ],\n",
    "        name=\"CNN\",\n",
    "    )\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "CNN().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"LSTM\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"LSTM\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">136</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │         \u001b[38;5;34m1,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_31 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_25 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │           \u001b[38;5;34m136\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_32 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_26 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m9\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,001</span> (7.82 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,001\u001b[0m (7.82 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,001</span> (7.82 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,001\u001b[0m (7.82 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def LSTM_MODEL(optimizer=\"adam\", units=16, activation=\"relu\", dropout_rate=0.2):\n",
    "    model = Sequential(name=\"LSTM\")\n",
    "\n",
    "    model.add(Input(shape=(X.shape[1], X.shape[2])))\n",
    "\n",
    "    # Segunda capa LSTM\n",
    "    model.add(\n",
    "        LSTM(\n",
    "            units,\n",
    "            activation=\"tanh\",\n",
    "            return_sequences=False,\n",
    "            kernel_regularizer=l2(0.03),\n",
    "        )\n",
    "    )\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(units // 2, activation=activation))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Capa de salida para clasificación binaria\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "LSTM_MODEL().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING = {\n",
    "    \"ff\": FF,\n",
    "    \"cnn\": CNN,\n",
    "    \"lstm\": LSTM_MODEL,\n",
    "}\n",
    "\n",
    "\n",
    "def save_fig(MODEL, FEATURE, timestamp):\n",
    "    ruta_carpeta = f\"{PLOTS_DIR}/{MODEL}_{FEATURE}\"\n",
    "    ruta_archivo = f\"{ruta_carpeta}/{timestamp}.png\"\n",
    "\n",
    "    # Crea la carpeta si no existe\n",
    "    os.makedirs(ruta_carpeta, exist_ok=True)  # exist_ok evita error si ya existe\n",
    "    plt.savefig(ruta_archivo)\n",
    "\n",
    "\n",
    "# Almacena las métricas para cada combinación de parámetros\n",
    "results = []\n",
    "\n",
    "# Lista de todas las combinaciones de parámetros\n",
    "param_combinations = list(\n",
    "    product(\n",
    "        [64, 128, 256],  # units\n",
    "        [0.1, 0.2, 0.3],  # dropout_rate\n",
    "        [\"relu\"],  # activation\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Probando combinación de parámetros: (64, 0.1, 'relu')\n",
      "Fold 1: \n",
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.4957 - loss: 0.7289 - val_accuracy: 0.4837 - val_loss: 0.6938\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5107 - loss: 0.6942 - val_accuracy: 0.4869 - val_loss: 0.6928\n",
      "Epoch 3/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5101 - loss: 0.6933 - val_accuracy: 0.5131 - val_loss: 0.6923\n",
      "Epoch 4/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4911 - loss: 0.6948 - val_accuracy: 0.4944 - val_loss: 0.6918\n",
      "Epoch 5/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5327 - loss: 0.6904 - val_accuracy: 0.5375 - val_loss: 0.6890\n",
      "Epoch 6/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5218 - loss: 0.6918 - val_accuracy: 0.5619 - val_loss: 0.6871\n",
      "Epoch 7/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5194 - loss: 0.6920 - val_accuracy: 0.5694 - val_loss: 0.6892\n",
      "Epoch 8/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5529 - loss: 0.6898 - val_accuracy: 0.5725 - val_loss: 0.6821\n",
      "Epoch 9/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5306 - loss: 0.6906 - val_accuracy: 0.5638 - val_loss: 0.6840\n",
      "Epoch 10/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5578 - loss: 0.6843 - val_accuracy: 0.5913 - val_loss: 0.6804\n",
      "Epoch 11/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5493 - loss: 0.6835 - val_accuracy: 0.5894 - val_loss: 0.6792\n",
      "Epoch 12/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5580 - loss: 0.6840 - val_accuracy: 0.5975 - val_loss: 0.6742\n",
      "Epoch 13/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5698 - loss: 0.6818 - val_accuracy: 0.5919 - val_loss: 0.6741\n",
      "Epoch 14/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5695 - loss: 0.6811 - val_accuracy: 0.5756 - val_loss: 0.6755\n",
      "Epoch 15/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5852 - loss: 0.6774 - val_accuracy: 0.5831 - val_loss: 0.6693\n",
      "Epoch 16/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5899 - loss: 0.6714 - val_accuracy: 0.5537 - val_loss: 0.6845\n",
      "Epoch 17/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5831 - loss: 0.6753 - val_accuracy: 0.5850 - val_loss: 0.6662\n",
      "Epoch 18/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5696 - loss: 0.6777 - val_accuracy: 0.6069 - val_loss: 0.6643\n",
      "Epoch 19/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5788 - loss: 0.6820 - val_accuracy: 0.5975 - val_loss: 0.6694\n",
      "Epoch 20/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5827 - loss: 0.6737 - val_accuracy: 0.6069 - val_loss: 0.6663\n",
      "Epoch 21/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5752 - loss: 0.6751 - val_accuracy: 0.6062 - val_loss: 0.6656\n",
      "Epoch 22/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5782 - loss: 0.6702 - val_accuracy: 0.5663 - val_loss: 0.6793\n",
      "Epoch 23/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6055 - loss: 0.6697 - val_accuracy: 0.6075 - val_loss: 0.6632\n",
      "Epoch 24/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5976 - loss: 0.6668 - val_accuracy: 0.6112 - val_loss: 0.6583\n",
      "Epoch 25/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5976 - loss: 0.6692 - val_accuracy: 0.6006 - val_loss: 0.6664\n",
      "Epoch 26/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6044 - loss: 0.6647 - val_accuracy: 0.6237 - val_loss: 0.6548\n",
      "Epoch 27/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6074 - loss: 0.6643 - val_accuracy: 0.6275 - val_loss: 0.6549\n",
      "Epoch 28/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6014 - loss: 0.6632 - val_accuracy: 0.5813 - val_loss: 0.6749\n",
      "Epoch 29/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6055 - loss: 0.6588 - val_accuracy: 0.6269 - val_loss: 0.6589\n",
      "Epoch 30/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6079 - loss: 0.6592 - val_accuracy: 0.6050 - val_loss: 0.6524\n",
      "Epoch 31/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6208 - loss: 0.6569 - val_accuracy: 0.6306 - val_loss: 0.6439\n",
      "Epoch 32/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6194 - loss: 0.6474 - val_accuracy: 0.6231 - val_loss: 0.6617\n",
      "Epoch 33/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6221 - loss: 0.6482 - val_accuracy: 0.6275 - val_loss: 0.6453\n",
      "Epoch 34/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5931 - loss: 0.6710 - val_accuracy: 0.6300 - val_loss: 0.6517\n",
      "Epoch 35/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5839 - loss: 0.6647 - val_accuracy: 0.6363 - val_loss: 0.6428\n",
      "Epoch 36/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6295 - loss: 0.6399 - val_accuracy: 0.6194 - val_loss: 0.6562\n",
      "Epoch 37/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6277 - loss: 0.6483 - val_accuracy: 0.6456 - val_loss: 0.6365\n",
      "Epoch 38/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6290 - loss: 0.6418 - val_accuracy: 0.6225 - val_loss: 0.6578\n",
      "Epoch 39/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6388 - loss: 0.6451 - val_accuracy: 0.5475 - val_loss: 0.6781\n",
      "Epoch 40/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6111 - loss: 0.6542 - val_accuracy: 0.5975 - val_loss: 0.6589\n",
      "Epoch 41/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5952 - loss: 0.6600 - val_accuracy: 0.6056 - val_loss: 0.6569\n",
      "Fold 1 - Pérdida de validación: 0.63648\n",
      "Fold 1 - Accuracy de validación: 0.64562\n",
      "Fold 2: \n",
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.5153 - loss: 0.7128 - val_accuracy: 0.5138 - val_loss: 0.6914\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5018 - loss: 0.6954 - val_accuracy: 0.5156 - val_loss: 0.6919\n",
      "Epoch 3/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5020 - loss: 0.6938 - val_accuracy: 0.5288 - val_loss: 0.6920\n",
      "Epoch 4/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5273 - loss: 0.6919 - val_accuracy: 0.5038 - val_loss: 0.6914\n",
      "Epoch 5/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5321 - loss: 0.6902 - val_accuracy: 0.5106 - val_loss: 0.6885\n",
      "Epoch 6/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5308 - loss: 0.6885 - val_accuracy: 0.6075 - val_loss: 0.6828\n",
      "Epoch 7/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5408 - loss: 0.6885 - val_accuracy: 0.5562 - val_loss: 0.6843\n",
      "Epoch 8/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5686 - loss: 0.6844 - val_accuracy: 0.5537 - val_loss: 0.6815\n",
      "Epoch 9/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5487 - loss: 0.6852 - val_accuracy: 0.6106 - val_loss: 0.6851\n",
      "Epoch 10/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.5297 - loss: 0.6864 - val_accuracy: 0.5856 - val_loss: 0.6814\n",
      "Epoch 11/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5500 - loss: 0.6850 - val_accuracy: 0.5906 - val_loss: 0.6819\n",
      "Epoch 12/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.5378 - loss: 0.6873 - val_accuracy: 0.5575 - val_loss: 0.6804\n",
      "Epoch 13/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5614 - loss: 0.6810 - val_accuracy: 0.6212 - val_loss: 0.6726\n",
      "Epoch 14/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5739 - loss: 0.6812 - val_accuracy: 0.6200 - val_loss: 0.6769\n",
      "Epoch 15/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5761 - loss: 0.6812 - val_accuracy: 0.5612 - val_loss: 0.6778\n",
      "Epoch 16/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5876 - loss: 0.6746 - val_accuracy: 0.5487 - val_loss: 0.6839\n",
      "Epoch 17/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5770 - loss: 0.6764 - val_accuracy: 0.5981 - val_loss: 0.6736\n",
      "Epoch 18/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5882 - loss: 0.6777 - val_accuracy: 0.6006 - val_loss: 0.6685\n",
      "Epoch 19/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6036 - loss: 0.6709 - val_accuracy: 0.5625 - val_loss: 0.6808\n",
      "Epoch 20/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5788 - loss: 0.6786 - val_accuracy: 0.6062 - val_loss: 0.6665\n",
      "Epoch 21/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5870 - loss: 0.6770 - val_accuracy: 0.6044 - val_loss: 0.6707\n",
      "Epoch 22/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5895 - loss: 0.6724 - val_accuracy: 0.5794 - val_loss: 0.6718\n",
      "Epoch 23/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5674 - loss: 0.6816 - val_accuracy: 0.6200 - val_loss: 0.6657\n",
      "Epoch 24/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5688 - loss: 0.6806 - val_accuracy: 0.6187 - val_loss: 0.6671\n",
      "Epoch 25/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5960 - loss: 0.6708 - val_accuracy: 0.6331 - val_loss: 0.6606\n",
      "Epoch 26/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6095 - loss: 0.6661 - val_accuracy: 0.5763 - val_loss: 0.6716\n",
      "Epoch 27/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5803 - loss: 0.6736 - val_accuracy: 0.6269 - val_loss: 0.6598\n",
      "Epoch 28/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5866 - loss: 0.6723 - val_accuracy: 0.6325 - val_loss: 0.6577\n",
      "Epoch 29/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5919 - loss: 0.6691 - val_accuracy: 0.6137 - val_loss: 0.6622\n",
      "Epoch 30/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5941 - loss: 0.6705 - val_accuracy: 0.6306 - val_loss: 0.6578\n",
      "Epoch 31/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5967 - loss: 0.6661 - val_accuracy: 0.6237 - val_loss: 0.6593\n",
      "Epoch 32/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5986 - loss: 0.6670 - val_accuracy: 0.6425 - val_loss: 0.6543\n",
      "Epoch 33/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6063 - loss: 0.6683 - val_accuracy: 0.6162 - val_loss: 0.6574\n",
      "Epoch 34/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5970 - loss: 0.6720 - val_accuracy: 0.6069 - val_loss: 0.6597\n",
      "Epoch 35/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5957 - loss: 0.6676 - val_accuracy: 0.5856 - val_loss: 0.6714\n",
      "Epoch 36/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5935 - loss: 0.6680 - val_accuracy: 0.5763 - val_loss: 0.6741\n",
      "Epoch 37/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6112 - loss: 0.6655 - val_accuracy: 0.6500 - val_loss: 0.6497\n",
      "Epoch 38/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5986 - loss: 0.6682 - val_accuracy: 0.6125 - val_loss: 0.6583\n",
      "Fold 2 - Pérdida de validación: 0.64973\n",
      "Fold 2 - Accuracy de validación: 0.65000\n",
      "Fold 3: \n",
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.4916 - loss: 0.7069 - val_accuracy: 0.5362 - val_loss: 0.6926\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4962 - loss: 0.6946 - val_accuracy: 0.5256 - val_loss: 0.6912\n",
      "Epoch 3/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5266 - loss: 0.6907 - val_accuracy: 0.4950 - val_loss: 0.6927\n",
      "Epoch 4/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5344 - loss: 0.6905 - val_accuracy: 0.5081 - val_loss: 0.6922\n",
      "Epoch 5/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5151 - loss: 0.6926 - val_accuracy: 0.5544 - val_loss: 0.6900\n",
      "Epoch 6/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5256 - loss: 0.6920 - val_accuracy: 0.5469 - val_loss: 0.6869\n",
      "Epoch 7/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5387 - loss: 0.6914 - val_accuracy: 0.5100 - val_loss: 0.6903\n",
      "Epoch 8/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5217 - loss: 0.6877 - val_accuracy: 0.5200 - val_loss: 0.6888\n",
      "Epoch 9/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5188 - loss: 0.6903 - val_accuracy: 0.4906 - val_loss: 0.6937\n",
      "Epoch 10/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5205 - loss: 0.6892 - val_accuracy: 0.5487 - val_loss: 0.6891\n",
      "Epoch 11/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5520 - loss: 0.6873 - val_accuracy: 0.5800 - val_loss: 0.6834\n",
      "Fold 3 - Pérdida de validación: 0.68344\n",
      "Fold 3 - Accuracy de validación: 0.58000\n",
      "Fold 4: \n",
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5185 - loss: 0.6976 - val_accuracy: 0.4806 - val_loss: 0.6935\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5128 - loss: 0.6943 - val_accuracy: 0.5206 - val_loss: 0.6919\n",
      "Epoch 3/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5043 - loss: 0.6936 - val_accuracy: 0.4837 - val_loss: 0.6930\n",
      "Epoch 4/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5160 - loss: 0.6929 - val_accuracy: 0.4756 - val_loss: 0.6935\n",
      "Epoch 5/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5154 - loss: 0.6923 - val_accuracy: 0.4794 - val_loss: 0.6934\n",
      "Epoch 6/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5179 - loss: 0.6921 - val_accuracy: 0.5694 - val_loss: 0.6905\n",
      "Epoch 7/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5261 - loss: 0.6917 - val_accuracy: 0.5206 - val_loss: 0.6920\n",
      "Epoch 8/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4943 - loss: 0.6945 - val_accuracy: 0.4906 - val_loss: 0.6928\n",
      "Epoch 9/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5255 - loss: 0.6922 - val_accuracy: 0.5412 - val_loss: 0.6923\n",
      "Epoch 10/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5269 - loss: 0.6922 - val_accuracy: 0.4825 - val_loss: 0.6930\n",
      "Epoch 11/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5144 - loss: 0.6923 - val_accuracy: 0.4800 - val_loss: 0.6961\n",
      "Fold 4 - Pérdida de validación: 0.69051\n",
      "Fold 4 - Accuracy de validación: 0.56937\n",
      "Fold 5: \n",
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.5119 - loss: 0.7068 - val_accuracy: 0.5063 - val_loss: 0.6926\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4991 - loss: 0.6947 - val_accuracy: 0.5019 - val_loss: 0.6935\n",
      "Epoch 3/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5058 - loss: 0.6940 - val_accuracy: 0.5025 - val_loss: 0.6924\n",
      "Epoch 4/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5056 - loss: 0.6935 - val_accuracy: 0.5750 - val_loss: 0.6905\n",
      "Epoch 5/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5372 - loss: 0.6910 - val_accuracy: 0.5500 - val_loss: 0.6900\n",
      "Epoch 6/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5349 - loss: 0.6892 - val_accuracy: 0.5869 - val_loss: 0.6856\n",
      "Epoch 7/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5366 - loss: 0.6887 - val_accuracy: 0.5025 - val_loss: 0.6930\n",
      "Epoch 8/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4803 - loss: 0.6937 - val_accuracy: 0.5744 - val_loss: 0.6883\n",
      "Epoch 9/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5529 - loss: 0.6867 - val_accuracy: 0.5437 - val_loss: 0.6874\n",
      "Epoch 10/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 53ms/step - accuracy: 0.5251 - loss: 0.6907 - val_accuracy: 0.5281 - val_loss: 0.6875\n",
      "Epoch 11/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.5571 - loss: 0.6813 - val_accuracy: 0.5681 - val_loss: 0.6848\n",
      "Fold 5 - Pérdida de validación: 0.68484\n",
      "Fold 5 - Accuracy de validación: 0.56813\n",
      "Parámetros: (64, 0.1, 'relu'),                 Porcentaje: 0.5,                 Tiempo: 00:00:11,                 Mean Accuracy: 0.6026,                 Mean Loss: 0.6690\n",
      "\n",
      "Probando combinación de parámetros: (64, 0.2, 'relu')\n",
      "Fold 1: \n",
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.4913 - loss: 0.7076 - val_accuracy: 0.4863 - val_loss: 0.6954\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5272 - loss: 0.6939 - val_accuracy: 0.5138 - val_loss: 0.6930\n",
      "Epoch 3/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5017 - loss: 0.6944 - val_accuracy: 0.4863 - val_loss: 0.6935\n",
      "Epoch 4/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5028 - loss: 0.6950 - val_accuracy: 0.5194 - val_loss: 0.6910\n",
      "Epoch 5/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4977 - loss: 0.6957 - val_accuracy: 0.4863 - val_loss: 0.6937\n",
      "Epoch 6/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4932 - loss: 0.6947 - val_accuracy: 0.5456 - val_loss: 0.6912\n",
      "Epoch 7/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5129 - loss: 0.6928 - val_accuracy: 0.5219 - val_loss: 0.6907\n",
      "Epoch 8/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5245 - loss: 0.6916 - val_accuracy: 0.5106 - val_loss: 0.6920\n",
      "Epoch 9/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5294 - loss: 0.6922 - val_accuracy: 0.5225 - val_loss: 0.6891\n",
      "Epoch 10/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5237 - loss: 0.6919 - val_accuracy: 0.5025 - val_loss: 0.6924\n",
      "Epoch 11/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5003 - loss: 0.6939 - val_accuracy: 0.4825 - val_loss: 0.6930\n",
      "Fold 1 - Pérdida de validación: 0.68913\n",
      "Fold 1 - Accuracy de validación: 0.52250\n",
      "Fold 2: \n",
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.5116 - loss: 0.7051 - val_accuracy: 0.5600 - val_loss: 0.6915\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4888 - loss: 0.7004 - val_accuracy: 0.4888 - val_loss: 0.6932\n",
      "Epoch 3/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5068 - loss: 0.6931 - val_accuracy: 0.4375 - val_loss: 0.6934\n",
      "Epoch 4/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4916 - loss: 0.6936 - val_accuracy: 0.4769 - val_loss: 0.6935\n",
      "Epoch 5/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5172 - loss: 0.6921 - val_accuracy: 0.5562 - val_loss: 0.6927\n",
      "Epoch 6/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5088 - loss: 0.6930 - val_accuracy: 0.5962 - val_loss: 0.6903\n",
      "Epoch 7/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5238 - loss: 0.6912 - val_accuracy: 0.4863 - val_loss: 0.6947\n",
      "Epoch 8/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5056 - loss: 0.6926 - val_accuracy: 0.4856 - val_loss: 0.6923\n",
      "Epoch 9/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5118 - loss: 0.6920 - val_accuracy: 0.5931 - val_loss: 0.6915\n",
      "Epoch 10/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5184 - loss: 0.6921 - val_accuracy: 0.5881 - val_loss: 0.6905\n",
      "Epoch 11/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5554 - loss: 0.6911 - val_accuracy: 0.5013 - val_loss: 0.6898\n",
      "Fold 2 - Pérdida de validación: 0.68985\n",
      "Fold 2 - Accuracy de validación: 0.50125\n",
      "Fold 3: \n",
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.4963 - loss: 0.7009 - val_accuracy: 0.4913 - val_loss: 0.6934\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5007 - loss: 0.6930 - val_accuracy: 0.4913 - val_loss: 0.6935\n",
      "Epoch 3/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4843 - loss: 0.6954 - val_accuracy: 0.4631 - val_loss: 0.6932\n",
      "Epoch 4/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5064 - loss: 0.6949 - val_accuracy: 0.5063 - val_loss: 0.6930\n",
      "Epoch 5/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5029 - loss: 0.6956 - val_accuracy: 0.5281 - val_loss: 0.6922\n",
      "Epoch 6/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5210 - loss: 0.6930 - val_accuracy: 0.4988 - val_loss: 0.6920\n",
      "Epoch 7/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5306 - loss: 0.6936 - val_accuracy: 0.5412 - val_loss: 0.6915\n",
      "Epoch 8/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5432 - loss: 0.6903 - val_accuracy: 0.5094 - val_loss: 0.6925\n",
      "Epoch 9/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5021 - loss: 0.6933 - val_accuracy: 0.5088 - val_loss: 0.6925\n",
      "Epoch 10/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4884 - loss: 0.6929 - val_accuracy: 0.5088 - val_loss: 0.6931\n",
      "Epoch 11/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5078 - loss: 0.6931 - val_accuracy: 0.5088 - val_loss: 0.6931\n",
      "Fold 3 - Pérdida de validación: 0.69152\n",
      "Fold 3 - Accuracy de validación: 0.54125\n",
      "Fold 4: \n",
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.5053 - loss: 0.7110 - val_accuracy: 0.5038 - val_loss: 0.6932\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4996 - loss: 0.6946 - val_accuracy: 0.5581 - val_loss: 0.6928\n",
      "Epoch 3/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5052 - loss: 0.6931 - val_accuracy: 0.4806 - val_loss: 0.6932\n",
      "Epoch 4/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5244 - loss: 0.6926 - val_accuracy: 0.4881 - val_loss: 0.6930\n",
      "Epoch 5/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5155 - loss: 0.6923 - val_accuracy: 0.4863 - val_loss: 0.6929\n",
      "Epoch 6/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5356 - loss: 0.6916 - val_accuracy: 0.4806 - val_loss: 0.6943\n",
      "Epoch 7/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5174 - loss: 0.6914 - val_accuracy: 0.4931 - val_loss: 0.6920\n",
      "Epoch 8/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5150 - loss: 0.6901 - val_accuracy: 0.5206 - val_loss: 0.6927\n",
      "Epoch 9/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4977 - loss: 0.6936 - val_accuracy: 0.5206 - val_loss: 0.6930\n",
      "Epoch 10/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5063 - loss: 0.6933 - val_accuracy: 0.4794 - val_loss: 0.6935\n",
      "Epoch 11/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5104 - loss: 0.6930 - val_accuracy: 0.4794 - val_loss: 0.6936\n",
      "Fold 4 - Pérdida de validación: 0.69203\n",
      "Fold 4 - Accuracy de validación: 0.49312\n",
      "Fold 5: \n",
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.5038 - loss: 0.7060 - val_accuracy: 0.4981 - val_loss: 0.6957\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5133 - loss: 0.6963 - val_accuracy: 0.4981 - val_loss: 0.6932\n",
      "Epoch 3/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5102 - loss: 0.6942 - val_accuracy: 0.5019 - val_loss: 0.6927\n",
      "Epoch 4/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5159 - loss: 0.6919 - val_accuracy: 0.5219 - val_loss: 0.6926\n",
      "Epoch 5/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5089 - loss: 0.6923 - val_accuracy: 0.4988 - val_loss: 0.6906\n",
      "Epoch 6/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5363 - loss: 0.6894 - val_accuracy: 0.4975 - val_loss: 0.6909\n",
      "Epoch 7/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5101 - loss: 0.6928 - val_accuracy: 0.5025 - val_loss: 0.6931\n",
      "Epoch 8/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5290 - loss: 0.6928 - val_accuracy: 0.4875 - val_loss: 0.6930\n",
      "Epoch 9/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4954 - loss: 0.6929 - val_accuracy: 0.5312 - val_loss: 0.6927\n",
      "Epoch 10/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5419 - loss: 0.6922 - val_accuracy: 0.5350 - val_loss: 0.6910\n",
      "Epoch 11/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5469 - loss: 0.6891 - val_accuracy: 0.5788 - val_loss: 0.6895\n",
      "Fold 5 - Pérdida de validación: 0.68949\n",
      "Fold 5 - Accuracy de validación: 0.57875\n",
      "Parámetros: (64, 0.1, 'relu'),                 Porcentaje: 0.5,                 Tiempo: 00:00:11,                 Mean Accuracy: 0.6026,                 Mean Loss: 0.6690\n",
      "Parámetros: (64, 0.2, 'relu'),                 Porcentaje: 0.5,                 Tiempo: 00:00:07,                 Mean Accuracy: 0.5274,                 Mean Loss: 0.6904\n",
      "\n",
      "Probando combinación de parámetros: (64, 0.3, 'relu')\n",
      "Fold 1: \n",
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5074 - loss: 0.7223 - val_accuracy: 0.5169 - val_loss: 0.6927\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5159 - loss: 0.6932 - val_accuracy: 0.5244 - val_loss: 0.6930\n",
      "Epoch 3/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5004 - loss: 0.6932 - val_accuracy: 0.5138 - val_loss: 0.6930\n",
      "Epoch 4/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4830 - loss: 0.6932 - val_accuracy: 0.5138 - val_loss: 0.6931\n",
      "Epoch 5/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4940 - loss: 0.6931 - val_accuracy: 0.4863 - val_loss: 0.6955\n",
      "Epoch 6/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5195 - loss: 0.6920 - val_accuracy: 0.5706 - val_loss: 0.6930\n",
      "Epoch 7/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5113 - loss: 0.6932 - val_accuracy: 0.5138 - val_loss: 0.6931\n",
      "Epoch 8/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5117 - loss: 0.6930 - val_accuracy: 0.4863 - val_loss: 0.6932\n",
      "Epoch 9/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5177 - loss: 0.6929 - val_accuracy: 0.4863 - val_loss: 0.6932\n",
      "Epoch 10/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4973 - loss: 0.6931 - val_accuracy: 0.4863 - val_loss: 0.6934\n",
      "Epoch 11/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5092 - loss: 0.6931 - val_accuracy: 0.4863 - val_loss: 0.6933\n",
      "Fold 1 - Pérdida de validación: 0.69267\n",
      "Fold 1 - Accuracy de validación: 0.51688\n",
      "Fold 2: \n",
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.5085 - loss: 0.7158 - val_accuracy: 0.4869 - val_loss: 0.6929\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5002 - loss: 0.6951 - val_accuracy: 0.5831 - val_loss: 0.6920\n",
      "Epoch 3/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5097 - loss: 0.6928 - val_accuracy: 0.5144 - val_loss: 0.6930\n",
      "Epoch 4/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4923 - loss: 0.6932 - val_accuracy: 0.5138 - val_loss: 0.6931\n",
      "Epoch 5/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4875 - loss: 0.6932 - val_accuracy: 0.5138 - val_loss: 0.6931\n",
      "Epoch 6/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5115 - loss: 0.6931 - val_accuracy: 0.4863 - val_loss: 0.6932\n",
      "Epoch 7/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5008 - loss: 0.6932 - val_accuracy: 0.4863 - val_loss: 0.6932\n",
      "Epoch 8/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4889 - loss: 0.6932 - val_accuracy: 0.4863 - val_loss: 0.6932\n",
      "Epoch 9/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4837 - loss: 0.6933 - val_accuracy: 0.4863 - val_loss: 0.6932\n",
      "Epoch 10/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4844 - loss: 0.6932 - val_accuracy: 0.4863 - val_loss: 0.6933\n",
      "Epoch 11/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4789 - loss: 0.6933 - val_accuracy: 0.4863 - val_loss: 0.6932\n",
      "Fold 2 - Pérdida de validación: 0.69200\n",
      "Fold 2 - Accuracy de validación: 0.58312\n",
      "Fold 3: \n",
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.4865 - loss: 0.7085 - val_accuracy: 0.5088 - val_loss: 0.6930\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4889 - loss: 0.6964 - val_accuracy: 0.5125 - val_loss: 0.6930\n",
      "Epoch 3/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5184 - loss: 0.6930 - val_accuracy: 0.5025 - val_loss: 0.6931\n",
      "Epoch 4/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5138 - loss: 0.6930 - val_accuracy: 0.4981 - val_loss: 0.6927\n",
      "Epoch 5/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5053 - loss: 0.6929 - val_accuracy: 0.4925 - val_loss: 0.6930\n",
      "Epoch 6/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5152 - loss: 0.6930 - val_accuracy: 0.5125 - val_loss: 0.6924\n",
      "Epoch 7/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5326 - loss: 0.6921 - val_accuracy: 0.5100 - val_loss: 0.6921\n",
      "Epoch 8/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5365 - loss: 0.6915 - val_accuracy: 0.5650 - val_loss: 0.6907\n",
      "Epoch 9/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5124 - loss: 0.6925 - val_accuracy: 0.5088 - val_loss: 0.6930\n",
      "Epoch 10/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5053 - loss: 0.6929 - val_accuracy: 0.5088 - val_loss: 0.6931\n",
      "Epoch 11/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4891 - loss: 0.6933 - val_accuracy: 0.4913 - val_loss: 0.6932\n",
      "Fold 3 - Pérdida de validación: 0.69072\n",
      "Fold 3 - Accuracy de validación: 0.56500\n",
      "Fold 4: \n",
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5077 - loss: 0.7095 - val_accuracy: 0.5181 - val_loss: 0.6928\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4980 - loss: 0.6945 - val_accuracy: 0.4794 - val_loss: 0.6946\n",
      "Epoch 3/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5076 - loss: 0.6944 - val_accuracy: 0.4794 - val_loss: 0.6934\n",
      "Epoch 4/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5083 - loss: 0.6931 - val_accuracy: 0.4794 - val_loss: 0.6933\n",
      "Epoch 5/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5001 - loss: 0.6932 - val_accuracy: 0.4794 - val_loss: 0.6934\n",
      "Epoch 6/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5221 - loss: 0.6929 - val_accuracy: 0.4794 - val_loss: 0.6937\n",
      "Epoch 7/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5127 - loss: 0.6929 - val_accuracy: 0.4794 - val_loss: 0.6939\n",
      "Epoch 8/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5057 - loss: 0.6931 - val_accuracy: 0.4794 - val_loss: 0.6937\n",
      "Epoch 9/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5177 - loss: 0.6928 - val_accuracy: 0.4794 - val_loss: 0.6938\n",
      "Epoch 10/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5044 - loss: 0.6932 - val_accuracy: 0.4794 - val_loss: 0.6938\n",
      "Epoch 11/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5151 - loss: 0.6929 - val_accuracy: 0.4794 - val_loss: 0.6939\n",
      "Fold 4 - Pérdida de validación: 0.69281\n",
      "Fold 4 - Accuracy de validación: 0.51812\n",
      "Fold 5: \n",
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.4891 - loss: 0.7225 - val_accuracy: 0.5019 - val_loss: 0.6929\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5163 - loss: 0.6949 - val_accuracy: 0.4981 - val_loss: 0.6932\n",
      "Epoch 3/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5162 - loss: 0.6929 - val_accuracy: 0.4981 - val_loss: 0.6932\n",
      "Epoch 4/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4952 - loss: 0.6932 - val_accuracy: 0.4981 - val_loss: 0.6932\n",
      "Epoch 5/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5133 - loss: 0.6931 - val_accuracy: 0.4981 - val_loss: 0.6932\n",
      "Epoch 6/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5027 - loss: 0.6931 - val_accuracy: 0.4981 - val_loss: 0.6932\n",
      "Epoch 7/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5153 - loss: 0.6930 - val_accuracy: 0.4981 - val_loss: 0.6932\n",
      "Epoch 8/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4922 - loss: 0.6932 - val_accuracy: 0.4981 - val_loss: 0.6932\n",
      "Epoch 9/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4922 - loss: 0.6932 - val_accuracy: 0.4981 - val_loss: 0.6932\n",
      "Epoch 10/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5012 - loss: 0.6932 - val_accuracy: 0.4981 - val_loss: 0.6932\n",
      "Epoch 11/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5096 - loss: 0.6931 - val_accuracy: 0.4981 - val_loss: 0.6932\n",
      "Fold 5 - Pérdida de validación: 0.69292\n",
      "Fold 5 - Accuracy de validación: 0.50187\n",
      "Parámetros: (64, 0.1, 'relu'),                 Porcentaje: 0.5,                 Tiempo: 00:00:11,                 Mean Accuracy: 0.6026,                 Mean Loss: 0.6690\n",
      "Parámetros: (64, 0.2, 'relu'),                 Porcentaje: 0.5,                 Tiempo: 00:00:07,                 Mean Accuracy: 0.5274,                 Mean Loss: 0.6904\n",
      "Parámetros: (64, 0.3, 'relu'),                 Porcentaje: 0.5,                 Tiempo: 00:00:06,                 Mean Accuracy: 0.5370,                 Mean Loss: 0.6922\n",
      "\n",
      "Probando combinación de parámetros: (128, 0.1, 'relu')\n",
      "Fold 1: \n",
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.5055 - loss: 0.7353 - val_accuracy: 0.5213 - val_loss: 0.6890\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5135 - loss: 0.7002 - val_accuracy: 0.5575 - val_loss: 0.6858\n",
      "Epoch 3/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5342 - loss: 0.6919 - val_accuracy: 0.5144 - val_loss: 0.6976\n",
      "Epoch 4/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5234 - loss: 0.6938 - val_accuracy: 0.5956 - val_loss: 0.6883\n",
      "Epoch 5/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5270 - loss: 0.6898 - val_accuracy: 0.5394 - val_loss: 0.6887\n",
      "Epoch 6/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5284 - loss: 0.6898 - val_accuracy: 0.5688 - val_loss: 0.6824\n",
      "Epoch 7/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5506 - loss: 0.6854 - val_accuracy: 0.5875 - val_loss: 0.6797\n",
      "Epoch 8/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5655 - loss: 0.6839 - val_accuracy: 0.5850 - val_loss: 0.6807\n",
      "Epoch 9/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5949 - loss: 0.6765 - val_accuracy: 0.5587 - val_loss: 0.6833\n",
      "Epoch 10/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5680 - loss: 0.6814 - val_accuracy: 0.5950 - val_loss: 0.6772\n",
      "Epoch 11/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5614 - loss: 0.6828 - val_accuracy: 0.5694 - val_loss: 0.6763\n",
      "Epoch 12/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5729 - loss: 0.6796 - val_accuracy: 0.5694 - val_loss: 0.6760\n",
      "Epoch 13/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5787 - loss: 0.6770 - val_accuracy: 0.6012 - val_loss: 0.6708\n",
      "Epoch 14/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5704 - loss: 0.6781 - val_accuracy: 0.5925 - val_loss: 0.6669\n",
      "Epoch 15/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5873 - loss: 0.6705 - val_accuracy: 0.6069 - val_loss: 0.6694\n",
      "Epoch 16/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6073 - loss: 0.6688 - val_accuracy: 0.6025 - val_loss: 0.6721\n",
      "Epoch 17/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5995 - loss: 0.6716 - val_accuracy: 0.5794 - val_loss: 0.6733\n",
      "Epoch 18/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6051 - loss: 0.6629 - val_accuracy: 0.5838 - val_loss: 0.6687\n",
      "Epoch 19/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6026 - loss: 0.6650 - val_accuracy: 0.6112 - val_loss: 0.6745\n",
      "Epoch 20/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5924 - loss: 0.6748 - val_accuracy: 0.6012 - val_loss: 0.6734\n",
      "Epoch 21/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5877 - loss: 0.6691 - val_accuracy: 0.5913 - val_loss: 0.6686\n",
      "Epoch 22/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5969 - loss: 0.6722 - val_accuracy: 0.6181 - val_loss: 0.6613\n",
      "Epoch 23/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5992 - loss: 0.6628 - val_accuracy: 0.6044 - val_loss: 0.6678\n",
      "Epoch 24/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5968 - loss: 0.6693 - val_accuracy: 0.6200 - val_loss: 0.6619\n",
      "Fold 1 - Pérdida de validación: 0.66129\n",
      "Fold 1 - Accuracy de validación: 0.61813\n",
      "Fold 2: \n",
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.4991 - loss: 0.7104 - val_accuracy: 0.5138 - val_loss: 0.6919\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5011 - loss: 0.6992 - val_accuracy: 0.5581 - val_loss: 0.6909\n",
      "Epoch 3/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5043 - loss: 0.6963 - val_accuracy: 0.4969 - val_loss: 0.6913\n",
      "Epoch 4/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5054 - loss: 0.6947 - val_accuracy: 0.5863 - val_loss: 0.6897\n",
      "Epoch 5/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5194 - loss: 0.6936 - val_accuracy: 0.4975 - val_loss: 0.6905\n",
      "Epoch 6/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5392 - loss: 0.6901 - val_accuracy: 0.6131 - val_loss: 0.6846\n",
      "Epoch 7/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5541 - loss: 0.6870 - val_accuracy: 0.5769 - val_loss: 0.6811\n",
      "Epoch 8/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5396 - loss: 0.6864 - val_accuracy: 0.6100 - val_loss: 0.6808\n",
      "Epoch 9/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5702 - loss: 0.6826 - val_accuracy: 0.6106 - val_loss: 0.6721\n",
      "Epoch 10/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5753 - loss: 0.6791 - val_accuracy: 0.5888 - val_loss: 0.6796\n",
      "Epoch 11/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5553 - loss: 0.6827 - val_accuracy: 0.5063 - val_loss: 0.6960\n",
      "Epoch 12/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5667 - loss: 0.6803 - val_accuracy: 0.6200 - val_loss: 0.6663\n",
      "Epoch 13/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5754 - loss: 0.6736 - val_accuracy: 0.5725 - val_loss: 0.6763\n",
      "Epoch 14/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5718 - loss: 0.6777 - val_accuracy: 0.6062 - val_loss: 0.6666\n",
      "Epoch 15/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5794 - loss: 0.6750 - val_accuracy: 0.6194 - val_loss: 0.6616\n",
      "Epoch 16/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5943 - loss: 0.6708 - val_accuracy: 0.6006 - val_loss: 0.6658\n",
      "Epoch 17/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5666 - loss: 0.6872 - val_accuracy: 0.6162 - val_loss: 0.6617\n",
      "Epoch 18/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6017 - loss: 0.6679 - val_accuracy: 0.6237 - val_loss: 0.6622\n",
      "Epoch 19/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5930 - loss: 0.6698 - val_accuracy: 0.6100 - val_loss: 0.6671\n",
      "Epoch 20/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5848 - loss: 0.6714 - val_accuracy: 0.6150 - val_loss: 0.6595\n",
      "Epoch 21/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5555 - loss: 0.6812 - val_accuracy: 0.6187 - val_loss: 0.6605\n",
      "Epoch 22/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5875 - loss: 0.6713 - val_accuracy: 0.6031 - val_loss: 0.6639\n",
      "Fold 2 - Pérdida de validación: 0.65951\n",
      "Fold 2 - Accuracy de validación: 0.61500\n",
      "Fold 3: \n",
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.4946 - loss: 0.7120 - val_accuracy: 0.4944 - val_loss: 0.6930\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 101ms/step - accuracy: 0.5252 - loss: 0.6951 - val_accuracy: 0.5088 - val_loss: 0.6928\n",
      "Epoch 3/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5148 - loss: 0.6938 - val_accuracy: 0.5294 - val_loss: 0.6916\n",
      "Epoch 4/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5311 - loss: 0.6924 - val_accuracy: 0.5769 - val_loss: 0.6864\n",
      "Epoch 5/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5290 - loss: 0.6919 - val_accuracy: 0.4950 - val_loss: 0.6916\n",
      "Epoch 6/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.5515 - loss: 0.6887 - val_accuracy: 0.5494 - val_loss: 0.6873\n",
      "Epoch 7/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5286 - loss: 0.6904 - val_accuracy: 0.5681 - val_loss: 0.6856\n",
      "Epoch 8/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.5637 - loss: 0.6821 - val_accuracy: 0.5450 - val_loss: 0.6867\n",
      "Epoch 9/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.5555 - loss: 0.6843 - val_accuracy: 0.4931 - val_loss: 0.6966\n",
      "Epoch 10/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.5479 - loss: 0.6874 - val_accuracy: 0.5644 - val_loss: 0.6852\n",
      "Epoch 11/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.5634 - loss: 0.6805 - val_accuracy: 0.5713 - val_loss: 0.6843\n",
      "Fold 3 - Pérdida de validación: 0.68430\n",
      "Fold 3 - Accuracy de validación: 0.57125\n",
      "Fold 4: \n",
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.5246 - loss: 0.7048 - val_accuracy: 0.5294 - val_loss: 0.6909\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5314 - loss: 0.6925 - val_accuracy: 0.5206 - val_loss: 0.6930\n",
      "Epoch 3/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4918 - loss: 0.6967 - val_accuracy: 0.5206 - val_loss: 0.6919\n",
      "Epoch 4/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5040 - loss: 0.6940 - val_accuracy: 0.5394 - val_loss: 0.6865\n",
      "Epoch 5/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5250 - loss: 0.6908 - val_accuracy: 0.5300 - val_loss: 0.6887\n",
      "Epoch 6/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5266 - loss: 0.6915 - val_accuracy: 0.5769 - val_loss: 0.6833\n",
      "Epoch 7/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5551 - loss: 0.6867 - val_accuracy: 0.6106 - val_loss: 0.6774\n",
      "Epoch 8/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5860 - loss: 0.6797 - val_accuracy: 0.5869 - val_loss: 0.6763\n",
      "Epoch 9/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5727 - loss: 0.6796 - val_accuracy: 0.6175 - val_loss: 0.6672\n",
      "Epoch 10/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5593 - loss: 0.6839 - val_accuracy: 0.6056 - val_loss: 0.6848\n",
      "Epoch 11/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5689 - loss: 0.6817 - val_accuracy: 0.6087 - val_loss: 0.6735\n",
      "Epoch 12/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5761 - loss: 0.6756 - val_accuracy: 0.6219 - val_loss: 0.6610\n",
      "Epoch 13/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5794 - loss: 0.6760 - val_accuracy: 0.6275 - val_loss: 0.6582\n",
      "Epoch 14/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5959 - loss: 0.6696 - val_accuracy: 0.6231 - val_loss: 0.6562\n",
      "Epoch 15/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6108 - loss: 0.6588 - val_accuracy: 0.5856 - val_loss: 0.6698\n",
      "Epoch 16/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5789 - loss: 0.6772 - val_accuracy: 0.6144 - val_loss: 0.6674\n",
      "Epoch 17/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5805 - loss: 0.6798 - val_accuracy: 0.6206 - val_loss: 0.6615\n",
      "Epoch 18/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6009 - loss: 0.6697 - val_accuracy: 0.5919 - val_loss: 0.6637\n",
      "Epoch 19/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5955 - loss: 0.6740 - val_accuracy: 0.6031 - val_loss: 0.6678\n",
      "Epoch 20/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5910 - loss: 0.6706 - val_accuracy: 0.6356 - val_loss: 0.6544\n",
      "Epoch 21/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5999 - loss: 0.6684 - val_accuracy: 0.6338 - val_loss: 0.6544\n",
      "Epoch 22/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5996 - loss: 0.6674 - val_accuracy: 0.6237 - val_loss: 0.6569\n",
      "Epoch 23/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5995 - loss: 0.6712 - val_accuracy: 0.6381 - val_loss: 0.6537\n",
      "Epoch 24/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6024 - loss: 0.6683 - val_accuracy: 0.6187 - val_loss: 0.6526\n",
      "Fold 4 - Pérdida de validación: 0.65260\n",
      "Fold 4 - Accuracy de validación: 0.61875\n",
      "Fold 5: \n",
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.5035 - loss: 0.7045 - val_accuracy: 0.5744 - val_loss: 0.6912\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4844 - loss: 0.6987 - val_accuracy: 0.5031 - val_loss: 0.6924\n",
      "Epoch 3/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5162 - loss: 0.6936 - val_accuracy: 0.5019 - val_loss: 0.6929\n",
      "Epoch 4/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5074 - loss: 0.6936 - val_accuracy: 0.5019 - val_loss: 0.6928\n",
      "Epoch 5/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4969 - loss: 0.6955 - val_accuracy: 0.4981 - val_loss: 0.6931\n",
      "Epoch 6/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4964 - loss: 0.6941 - val_accuracy: 0.5038 - val_loss: 0.6931\n",
      "Epoch 7/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5068 - loss: 0.6922 - val_accuracy: 0.5806 - val_loss: 0.6913\n",
      "Epoch 8/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5291 - loss: 0.6920 - val_accuracy: 0.5725 - val_loss: 0.6899\n",
      "Epoch 9/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5393 - loss: 0.6912 - val_accuracy: 0.5562 - val_loss: 0.6892\n",
      "Epoch 10/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5475 - loss: 0.6889 - val_accuracy: 0.5763 - val_loss: 0.6883\n",
      "Epoch 11/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5670 - loss: 0.6851 - val_accuracy: 0.5744 - val_loss: 0.6806\n",
      "Epoch 12/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5804 - loss: 0.6762 - val_accuracy: 0.5881 - val_loss: 0.6845\n",
      "Epoch 13/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5597 - loss: 0.6828 - val_accuracy: 0.5581 - val_loss: 0.6854\n",
      "Epoch 14/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5417 - loss: 0.6871 - val_accuracy: 0.5738 - val_loss: 0.6808\n",
      "Epoch 15/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5798 - loss: 0.6800 - val_accuracy: 0.5944 - val_loss: 0.6771\n",
      "Epoch 16/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5802 - loss: 0.6784 - val_accuracy: 0.5800 - val_loss: 0.6762\n",
      "Epoch 17/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5883 - loss: 0.6772 - val_accuracy: 0.5944 - val_loss: 0.6751\n",
      "Epoch 18/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5763 - loss: 0.6746 - val_accuracy: 0.5844 - val_loss: 0.6783\n",
      "Epoch 19/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5675 - loss: 0.6779 - val_accuracy: 0.5331 - val_loss: 0.6870\n",
      "Epoch 20/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5774 - loss: 0.6795 - val_accuracy: 0.5813 - val_loss: 0.6758\n",
      "Epoch 21/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5718 - loss: 0.6781 - val_accuracy: 0.5950 - val_loss: 0.6740\n",
      "Fold 5 - Pérdida de validación: 0.67404\n",
      "Fold 5 - Accuracy de validación: 0.59500\n",
      "Parámetros: (64, 0.1, 'relu'),                 Porcentaje: 0.5,                 Tiempo: 00:00:11,                 Mean Accuracy: 0.6026,                 Mean Loss: 0.6690\n",
      "Parámetros: (64, 0.2, 'relu'),                 Porcentaje: 0.5,                 Tiempo: 00:00:07,                 Mean Accuracy: 0.5274,                 Mean Loss: 0.6904\n",
      "Parámetros: (64, 0.3, 'relu'),                 Porcentaje: 0.5,                 Tiempo: 00:00:06,                 Mean Accuracy: 0.5370,                 Mean Loss: 0.6922\n",
      "Parámetros: (128, 0.1, 'relu'),                 Porcentaje: 0.5,                 Tiempo: 00:00:14,                 Mean Accuracy: 0.6036,                 Mean Loss: 0.6663\n",
      "\n",
      "Probando combinación de parámetros: (128, 0.2, 'relu')\n",
      "Fold 1: \n",
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.5157 - loss: 0.7224 - val_accuracy: 0.5138 - val_loss: 0.6913\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5050 - loss: 0.6969 - val_accuracy: 0.5138 - val_loss: 0.6918\n",
      "Epoch 3/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5115 - loss: 0.6960 - val_accuracy: 0.5300 - val_loss: 0.6926\n",
      "Epoch 4/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5052 - loss: 0.6956 - val_accuracy: 0.5138 - val_loss: 0.6922\n",
      "Epoch 5/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4975 - loss: 0.6951 - val_accuracy: 0.5138 - val_loss: 0.6922\n",
      "Epoch 6/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5103 - loss: 0.6921 - val_accuracy: 0.4863 - val_loss: 0.6934\n",
      "Epoch 7/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4966 - loss: 0.6938 - val_accuracy: 0.4863 - val_loss: 0.6936\n",
      "Epoch 8/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5170 - loss: 0.6927 - val_accuracy: 0.5944 - val_loss: 0.6913\n",
      "Epoch 9/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5139 - loss: 0.6930 - val_accuracy: 0.4863 - val_loss: 0.6934\n",
      "Epoch 10/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5041 - loss: 0.6930 - val_accuracy: 0.4863 - val_loss: 0.6929\n",
      "Epoch 11/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4924 - loss: 0.6907 - val_accuracy: 0.5475 - val_loss: 0.6910\n",
      "Fold 1 - Pérdida de validación: 0.69104\n",
      "Fold 1 - Accuracy de validación: 0.54750\n",
      "Fold 2: \n",
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.4809 - loss: 0.7185 - val_accuracy: 0.5138 - val_loss: 0.6925\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5098 - loss: 0.6983 - val_accuracy: 0.5138 - val_loss: 0.6928\n",
      "Epoch 3/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4989 - loss: 0.7014 - val_accuracy: 0.5419 - val_loss: 0.6921\n",
      "Epoch 4/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5139 - loss: 0.6933 - val_accuracy: 0.4863 - val_loss: 0.6940\n",
      "Epoch 5/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5087 - loss: 0.6937 - val_accuracy: 0.4863 - val_loss: 0.6944\n",
      "Epoch 6/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5124 - loss: 0.6932 - val_accuracy: 0.4863 - val_loss: 0.6933\n",
      "Epoch 7/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5025 - loss: 0.6935 - val_accuracy: 0.4863 - val_loss: 0.6931\n",
      "Epoch 8/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5064 - loss: 0.6932 - val_accuracy: 0.4863 - val_loss: 0.6932\n",
      "Epoch 9/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5018 - loss: 0.6931 - val_accuracy: 0.4863 - val_loss: 0.6932\n",
      "Epoch 10/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5027 - loss: 0.6931 - val_accuracy: 0.4863 - val_loss: 0.6933\n",
      "Epoch 11/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5180 - loss: 0.6930 - val_accuracy: 0.4863 - val_loss: 0.6933\n",
      "Fold 2 - Pérdida de validación: 0.69212\n",
      "Fold 2 - Accuracy de validación: 0.54188\n",
      "Fold 3: \n",
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.5004 - loss: 0.7242 - val_accuracy: 0.4762 - val_loss: 0.6934\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5007 - loss: 0.6954 - val_accuracy: 0.5088 - val_loss: 0.6931\n",
      "Epoch 3/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4951 - loss: 0.6960 - val_accuracy: 0.5069 - val_loss: 0.6930\n",
      "Epoch 4/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5094 - loss: 0.6947 - val_accuracy: 0.4925 - val_loss: 0.6932\n",
      "Epoch 5/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5033 - loss: 0.6931 - val_accuracy: 0.4950 - val_loss: 0.6931\n",
      "Epoch 6/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5071 - loss: 0.6929 - val_accuracy: 0.4913 - val_loss: 0.6932\n",
      "Epoch 7/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5147 - loss: 0.6938 - val_accuracy: 0.5244 - val_loss: 0.6929\n",
      "Epoch 8/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5182 - loss: 0.6927 - val_accuracy: 0.4913 - val_loss: 0.6932\n",
      "Epoch 9/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4889 - loss: 0.6942 - val_accuracy: 0.5000 - val_loss: 0.6928\n",
      "Epoch 10/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5194 - loss: 0.6928 - val_accuracy: 0.4925 - val_loss: 0.6931\n",
      "Epoch 11/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4823 - loss: 0.6943 - val_accuracy: 0.4956 - val_loss: 0.6931\n",
      "Fold 3 - Pérdida de validación: 0.69283\n",
      "Fold 3 - Accuracy de validación: 0.50000\n",
      "Fold 4: \n",
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5066 - loss: 0.7188 - val_accuracy: 0.5213 - val_loss: 0.6902\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5101 - loss: 0.6960 - val_accuracy: 0.4812 - val_loss: 0.6942\n",
      "Epoch 3/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5214 - loss: 0.6942 - val_accuracy: 0.5219 - val_loss: 0.6893\n",
      "Epoch 4/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5303 - loss: 0.6907 - val_accuracy: 0.5700 - val_loss: 0.6880\n",
      "Epoch 5/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5139 - loss: 0.6935 - val_accuracy: 0.4831 - val_loss: 0.6932\n",
      "Epoch 6/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5146 - loss: 0.6923 - val_accuracy: 0.5612 - val_loss: 0.6900\n",
      "Epoch 7/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5146 - loss: 0.6929 - val_accuracy: 0.5200 - val_loss: 0.6893\n",
      "Epoch 8/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5339 - loss: 0.6896 - val_accuracy: 0.6156 - val_loss: 0.6891\n",
      "Epoch 9/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5357 - loss: 0.6902 - val_accuracy: 0.4875 - val_loss: 0.6941\n",
      "Epoch 10/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5284 - loss: 0.6904 - val_accuracy: 0.5981 - val_loss: 0.6872\n",
      "Epoch 11/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5204 - loss: 0.6908 - val_accuracy: 0.6056 - val_loss: 0.6787\n",
      "Epoch 12/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5461 - loss: 0.6872 - val_accuracy: 0.5969 - val_loss: 0.6751\n",
      "Epoch 13/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5607 - loss: 0.6865 - val_accuracy: 0.5744 - val_loss: 0.6799\n",
      "Epoch 14/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5601 - loss: 0.6826 - val_accuracy: 0.6144 - val_loss: 0.6677\n",
      "Epoch 15/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5336 - loss: 0.6884 - val_accuracy: 0.5406 - val_loss: 0.6897\n",
      "Epoch 16/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5497 - loss: 0.6884 - val_accuracy: 0.5969 - val_loss: 0.6781\n",
      "Epoch 17/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5825 - loss: 0.6774 - val_accuracy: 0.5925 - val_loss: 0.6757\n",
      "Epoch 18/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5587 - loss: 0.6852 - val_accuracy: 0.6206 - val_loss: 0.6637\n",
      "Epoch 19/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5613 - loss: 0.6870 - val_accuracy: 0.5744 - val_loss: 0.6783\n",
      "Epoch 20/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5832 - loss: 0.6778 - val_accuracy: 0.5631 - val_loss: 0.6797\n",
      "Epoch 21/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5572 - loss: 0.6849 - val_accuracy: 0.5744 - val_loss: 0.6777\n",
      "Epoch 22/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5872 - loss: 0.6756 - val_accuracy: 0.6219 - val_loss: 0.6613\n",
      "Epoch 23/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5850 - loss: 0.6759 - val_accuracy: 0.5925 - val_loss: 0.6733\n",
      "Epoch 24/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5922 - loss: 0.6751 - val_accuracy: 0.6250 - val_loss: 0.6565\n",
      "Epoch 25/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5803 - loss: 0.6767 - val_accuracy: 0.6331 - val_loss: 0.6582\n",
      "Epoch 26/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5703 - loss: 0.6838 - val_accuracy: 0.6375 - val_loss: 0.6579\n",
      "Epoch 27/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5803 - loss: 0.6763 - val_accuracy: 0.6313 - val_loss: 0.6573\n",
      "Epoch 28/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5739 - loss: 0.6817 - val_accuracy: 0.6306 - val_loss: 0.6572\n",
      "Epoch 29/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5950 - loss: 0.6732 - val_accuracy: 0.6394 - val_loss: 0.6533\n",
      "Epoch 30/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5890 - loss: 0.6699 - val_accuracy: 0.6319 - val_loss: 0.6574\n",
      "Epoch 31/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6118 - loss: 0.6651 - val_accuracy: 0.5950 - val_loss: 0.6710\n",
      "Epoch 32/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5850 - loss: 0.6729 - val_accuracy: 0.6413 - val_loss: 0.6520\n",
      "Epoch 33/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5882 - loss: 0.6743 - val_accuracy: 0.6219 - val_loss: 0.6617\n",
      "Epoch 34/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6088 - loss: 0.6668 - val_accuracy: 0.6075 - val_loss: 0.6653\n",
      "Fold 4 - Pérdida de validación: 0.65201\n",
      "Fold 4 - Accuracy de validación: 0.64125\n",
      "Fold 5: \n",
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.5212 - loss: 0.7135 - val_accuracy: 0.4981 - val_loss: 0.6940\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5006 - loss: 0.6992 - val_accuracy: 0.4981 - val_loss: 0.6933\n",
      "Epoch 3/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5048 - loss: 0.6959 - val_accuracy: 0.4981 - val_loss: 0.6934\n",
      "Epoch 4/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5148 - loss: 0.6935 - val_accuracy: 0.5081 - val_loss: 0.6929\n",
      "Epoch 5/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5184 - loss: 0.6935 - val_accuracy: 0.5031 - val_loss: 0.6929\n",
      "Epoch 6/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5090 - loss: 0.6927 - val_accuracy: 0.4981 - val_loss: 0.6929\n",
      "Epoch 7/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5166 - loss: 0.6931 - val_accuracy: 0.5713 - val_loss: 0.6924\n",
      "Epoch 8/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5308 - loss: 0.6915 - val_accuracy: 0.5881 - val_loss: 0.6913\n",
      "Epoch 9/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5336 - loss: 0.6922 - val_accuracy: 0.5213 - val_loss: 0.6905\n",
      "Epoch 10/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5260 - loss: 0.6931 - val_accuracy: 0.4994 - val_loss: 0.6935\n",
      "Epoch 11/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5027 - loss: 0.6933 - val_accuracy: 0.4981 - val_loss: 0.6934\n",
      "Fold 5 - Pérdida de validación: 0.69045\n",
      "Fold 5 - Accuracy de validación: 0.52125\n",
      "Parámetros: (64, 0.1, 'relu'),                 Porcentaje: 0.5,                 Tiempo: 00:00:11,                 Mean Accuracy: 0.6026,                 Mean Loss: 0.6690\n",
      "Parámetros: (64, 0.2, 'relu'),                 Porcentaje: 0.5,                 Tiempo: 00:00:07,                 Mean Accuracy: 0.5274,                 Mean Loss: 0.6904\n",
      "Parámetros: (64, 0.3, 'relu'),                 Porcentaje: 0.5,                 Tiempo: 00:00:06,                 Mean Accuracy: 0.5370,                 Mean Loss: 0.6922\n",
      "Parámetros: (128, 0.1, 'relu'),                 Porcentaje: 0.5,                 Tiempo: 00:00:14,                 Mean Accuracy: 0.6036,                 Mean Loss: 0.6663\n",
      "Parámetros: (128, 0.2, 'relu'),                 Porcentaje: 0.5,                 Tiempo: 00:00:08,                 Mean Accuracy: 0.5504,                 Mean Loss: 0.6837\n",
      "\n",
      "Probando combinación de parámetros: (128, 0.3, 'relu')\n",
      "Fold 1: \n",
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.5019 - loss: 0.7118 - val_accuracy: 0.4863 - val_loss: 0.6946\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4922 - loss: 0.7001 - val_accuracy: 0.5131 - val_loss: 0.6932\n",
      "Epoch 3/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4897 - loss: 0.6968 - val_accuracy: 0.4863 - val_loss: 0.6963\n",
      "Epoch 4/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5064 - loss: 0.6950 - val_accuracy: 0.4888 - val_loss: 0.6932\n",
      "Epoch 5/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5000 - loss: 0.6931 - val_accuracy: 0.5106 - val_loss: 0.6931\n",
      "Epoch 6/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5083 - loss: 0.6930 - val_accuracy: 0.5138 - val_loss: 0.6930\n",
      "Epoch 7/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5148 - loss: 0.6925 - val_accuracy: 0.4863 - val_loss: 0.6932\n",
      "Epoch 8/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5256 - loss: 0.6929 - val_accuracy: 0.4863 - val_loss: 0.6934\n",
      "Epoch 9/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4912 - loss: 0.6933 - val_accuracy: 0.4863 - val_loss: 0.6932\n",
      "Epoch 10/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5053 - loss: 0.6931 - val_accuracy: 0.4863 - val_loss: 0.6933\n",
      "Epoch 11/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5150 - loss: 0.6930 - val_accuracy: 0.4863 - val_loss: 0.6933\n",
      "Fold 1 - Pérdida de validación: 0.69301\n",
      "Fold 1 - Accuracy de validación: 0.51375\n",
      "Fold 2: \n",
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.4856 - loss: 0.7126 - val_accuracy: 0.5931 - val_loss: 0.6916\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4895 - loss: 0.7022 - val_accuracy: 0.4863 - val_loss: 0.6946\n",
      "Epoch 3/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5262 - loss: 0.6943 - val_accuracy: 0.5138 - val_loss: 0.6927\n",
      "Epoch 4/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4984 - loss: 0.6953 - val_accuracy: 0.4844 - val_loss: 0.6931\n",
      "Epoch 5/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5144 - loss: 0.6930 - val_accuracy: 0.4837 - val_loss: 0.6928\n",
      "Epoch 6/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5152 - loss: 0.6926 - val_accuracy: 0.5800 - val_loss: 0.6907\n",
      "Epoch 7/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4958 - loss: 0.6933 - val_accuracy: 0.5775 - val_loss: 0.6904\n",
      "Epoch 8/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4983 - loss: 0.6936 - val_accuracy: 0.4863 - val_loss: 0.6942\n",
      "Epoch 9/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5080 - loss: 0.6933 - val_accuracy: 0.4863 - val_loss: 0.6935\n",
      "Epoch 10/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5190 - loss: 0.6926 - val_accuracy: 0.4863 - val_loss: 0.6934\n",
      "Epoch 11/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4955 - loss: 0.6931 - val_accuracy: 0.4863 - val_loss: 0.6934\n",
      "Fold 2 - Pérdida de validación: 0.69039\n",
      "Fold 2 - Accuracy de validación: 0.57750\n",
      "Fold 3: \n",
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.5103 - loss: 0.7232 - val_accuracy: 0.5088 - val_loss: 0.6929\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4934 - loss: 0.6985 - val_accuracy: 0.5088 - val_loss: 0.6930\n",
      "Epoch 3/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5099 - loss: 0.6960 - val_accuracy: 0.5088 - val_loss: 0.6930\n",
      "Epoch 4/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5088 - loss: 0.6939 - val_accuracy: 0.5088 - val_loss: 0.6931\n",
      "Epoch 5/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5044 - loss: 0.6934 - val_accuracy: 0.5088 - val_loss: 0.6930\n",
      "Epoch 6/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4914 - loss: 0.6937 - val_accuracy: 0.5088 - val_loss: 0.6930\n",
      "Epoch 7/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4859 - loss: 0.6934 - val_accuracy: 0.5088 - val_loss: 0.6931\n",
      "Epoch 8/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4942 - loss: 0.6933 - val_accuracy: 0.5088 - val_loss: 0.6931\n",
      "Epoch 9/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5138 - loss: 0.6930 - val_accuracy: 0.5088 - val_loss: 0.6931\n",
      "Epoch 10/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5037 - loss: 0.6931 - val_accuracy: 0.5088 - val_loss: 0.6931\n",
      "Epoch 11/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5162 - loss: 0.6930 - val_accuracy: 0.5088 - val_loss: 0.6931\n",
      "Fold 3 - Pérdida de validación: 0.69288\n",
      "Fold 3 - Accuracy de validación: 0.50875\n",
      "Fold 4: \n",
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.5064 - loss: 0.7244 - val_accuracy: 0.5206 - val_loss: 0.6923\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4935 - loss: 0.7004 - val_accuracy: 0.4794 - val_loss: 0.6945\n",
      "Epoch 3/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5079 - loss: 0.6927 - val_accuracy: 0.4794 - val_loss: 0.6931\n",
      "Epoch 4/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4880 - loss: 0.6921 - val_accuracy: 0.4806 - val_loss: 0.6934\n",
      "Epoch 5/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5015 - loss: 0.6919 - val_accuracy: 0.5925 - val_loss: 0.6922\n",
      "Epoch 6/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5278 - loss: 0.6919 - val_accuracy: 0.5206 - val_loss: 0.6930\n",
      "Epoch 7/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4831 - loss: 0.6932 - val_accuracy: 0.4794 - val_loss: 0.6932\n",
      "Epoch 8/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5144 - loss: 0.6931 - val_accuracy: 0.4794 - val_loss: 0.6936\n",
      "Epoch 9/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5088 - loss: 0.6930 - val_accuracy: 0.4794 - val_loss: 0.6936\n",
      "Epoch 10/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.4980 - loss: 0.6933 - val_accuracy: 0.4794 - val_loss: 0.6937\n",
      "Epoch 11/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5016 - loss: 0.6932 - val_accuracy: 0.4794 - val_loss: 0.6938\n",
      "Fold 4 - Pérdida de validación: 0.69217\n",
      "Fold 4 - Accuracy de validación: 0.59250\n",
      "Fold 5: \n",
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.4992 - loss: 0.7208 - val_accuracy: 0.4981 - val_loss: 0.6939\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5204 - loss: 0.6976 - val_accuracy: 0.5019 - val_loss: 0.6931\n",
      "Epoch 3/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5186 - loss: 0.6961 - val_accuracy: 0.5025 - val_loss: 0.6920\n",
      "Epoch 4/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5075 - loss: 0.6927 - val_accuracy: 0.5344 - val_loss: 0.6926\n",
      "Epoch 5/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4958 - loss: 0.6928 - val_accuracy: 0.5356 - val_loss: 0.6910\n",
      "Epoch 6/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5114 - loss: 0.6906 - val_accuracy: 0.4975 - val_loss: 0.6925\n",
      "Epoch 7/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5161 - loss: 0.6914 - val_accuracy: 0.5731 - val_loss: 0.6900\n",
      "Epoch 8/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5127 - loss: 0.6905 - val_accuracy: 0.5038 - val_loss: 0.6931\n",
      "Epoch 9/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4926 - loss: 0.6933 - val_accuracy: 0.5044 - val_loss: 0.6930\n",
      "Epoch 10/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5259 - loss: 0.6917 - val_accuracy: 0.5756 - val_loss: 0.6876\n",
      "Epoch 11/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5073 - loss: 0.6908 - val_accuracy: 0.5756 - val_loss: 0.6877\n",
      "Fold 5 - Pérdida de validación: 0.68755\n",
      "Fold 5 - Accuracy de validación: 0.57563\n",
      "Parámetros: (64, 0.1, 'relu'),                 Porcentaje: 0.5,                 Tiempo: 00:00:11,                 Mean Accuracy: 0.6026,                 Mean Loss: 0.6690\n",
      "Parámetros: (64, 0.2, 'relu'),                 Porcentaje: 0.5,                 Tiempo: 00:00:07,                 Mean Accuracy: 0.5274,                 Mean Loss: 0.6904\n",
      "Parámetros: (64, 0.3, 'relu'),                 Porcentaje: 0.5,                 Tiempo: 00:00:06,                 Mean Accuracy: 0.5370,                 Mean Loss: 0.6922\n",
      "Parámetros: (128, 0.1, 'relu'),                 Porcentaje: 0.5,                 Tiempo: 00:00:14,                 Mean Accuracy: 0.6036,                 Mean Loss: 0.6663\n",
      "Parámetros: (128, 0.2, 'relu'),                 Porcentaje: 0.5,                 Tiempo: 00:00:08,                 Mean Accuracy: 0.5504,                 Mean Loss: 0.6837\n",
      "Parámetros: (128, 0.3, 'relu'),                 Porcentaje: 0.5,                 Tiempo: 00:00:07,                 Mean Accuracy: 0.5536,                 Mean Loss: 0.6912\n",
      "\n",
      "Probando combinación de parámetros: (256, 0.1, 'relu')\n",
      "Fold 1: \n",
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.4898 - loss: 0.7362 - val_accuracy: 0.4831 - val_loss: 0.6927\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4837 - loss: 0.7006 - val_accuracy: 0.5181 - val_loss: 0.6897\n",
      "Epoch 3/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5127 - loss: 0.6926 - val_accuracy: 0.5525 - val_loss: 0.6901\n",
      "Epoch 4/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5291 - loss: 0.6941 - val_accuracy: 0.5425 - val_loss: 0.6899\n",
      "Epoch 5/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5240 - loss: 0.6908 - val_accuracy: 0.5625 - val_loss: 0.6871\n",
      "Epoch 6/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5415 - loss: 0.6913 - val_accuracy: 0.4794 - val_loss: 0.6940\n",
      "Epoch 7/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5038 - loss: 0.6951 - val_accuracy: 0.5269 - val_loss: 0.6890\n",
      "Epoch 8/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5167 - loss: 0.6939 - val_accuracy: 0.5606 - val_loss: 0.6869\n",
      "Epoch 9/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5418 - loss: 0.6879 - val_accuracy: 0.5688 - val_loss: 0.6860\n",
      "Epoch 10/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5751 - loss: 0.6846 - val_accuracy: 0.5319 - val_loss: 0.6895\n",
      "Epoch 11/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5438 - loss: 0.6891 - val_accuracy: 0.5675 - val_loss: 0.6851\n",
      "Fold 1 - Pérdida de validación: 0.68512\n",
      "Fold 1 - Accuracy de validación: 0.56750\n",
      "Fold 2: \n",
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.5099 - loss: 0.7284 - val_accuracy: 0.4863 - val_loss: 0.6953\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.4914 - loss: 0.7008 - val_accuracy: 0.4975 - val_loss: 0.6922\n",
      "Epoch 3/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5136 - loss: 0.6943 - val_accuracy: 0.4913 - val_loss: 0.6914\n",
      "Epoch 4/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4957 - loss: 0.6959 - val_accuracy: 0.6087 - val_loss: 0.6888\n",
      "Epoch 5/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5456 - loss: 0.6904 - val_accuracy: 0.5213 - val_loss: 0.6880\n",
      "Epoch 6/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5324 - loss: 0.6914 - val_accuracy: 0.5362 - val_loss: 0.6862\n",
      "Epoch 7/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5487 - loss: 0.6890 - val_accuracy: 0.5475 - val_loss: 0.6822\n",
      "Epoch 8/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5598 - loss: 0.6835 - val_accuracy: 0.5994 - val_loss: 0.6800\n",
      "Epoch 9/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5799 - loss: 0.6803 - val_accuracy: 0.5994 - val_loss: 0.6720\n",
      "Epoch 10/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5829 - loss: 0.6772 - val_accuracy: 0.5506 - val_loss: 0.6855\n",
      "Epoch 11/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5914 - loss: 0.6734 - val_accuracy: 0.5306 - val_loss: 0.6882\n",
      "Epoch 12/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5457 - loss: 0.6844 - val_accuracy: 0.6150 - val_loss: 0.6654\n",
      "Epoch 13/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5732 - loss: 0.6764 - val_accuracy: 0.5650 - val_loss: 0.6816\n",
      "Epoch 14/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5772 - loss: 0.6782 - val_accuracy: 0.5981 - val_loss: 0.6711\n",
      "Epoch 15/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5963 - loss: 0.6729 - val_accuracy: 0.5625 - val_loss: 0.6870\n",
      "Epoch 16/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5685 - loss: 0.6759 - val_accuracy: 0.5681 - val_loss: 0.6792\n",
      "Epoch 17/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5776 - loss: 0.6767 - val_accuracy: 0.5763 - val_loss: 0.6744\n",
      "Epoch 18/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5652 - loss: 0.6819 - val_accuracy: 0.5587 - val_loss: 0.6814\n",
      "Epoch 19/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5668 - loss: 0.6803 - val_accuracy: 0.5625 - val_loss: 0.6797\n",
      "Fold 2 - Pérdida de validación: 0.66543\n",
      "Fold 2 - Accuracy de validación: 0.61500\n",
      "Fold 3: \n",
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.5096 - loss: 0.7098 - val_accuracy: 0.4913 - val_loss: 0.7028\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5114 - loss: 0.6981 - val_accuracy: 0.5537 - val_loss: 0.6904\n",
      "Epoch 3/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5025 - loss: 0.6965 - val_accuracy: 0.4975 - val_loss: 0.6914\n",
      "Epoch 4/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5264 - loss: 0.6917 - val_accuracy: 0.5706 - val_loss: 0.6874\n",
      "Epoch 5/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5441 - loss: 0.6863 - val_accuracy: 0.5731 - val_loss: 0.6844\n",
      "Epoch 6/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5589 - loss: 0.6835 - val_accuracy: 0.5775 - val_loss: 0.6823\n",
      "Epoch 7/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5670 - loss: 0.6779 - val_accuracy: 0.5106 - val_loss: 0.6985\n",
      "Epoch 8/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5083 - loss: 0.6952 - val_accuracy: 0.5494 - val_loss: 0.6953\n",
      "Epoch 9/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5457 - loss: 0.6893 - val_accuracy: 0.5794 - val_loss: 0.6787\n",
      "Epoch 10/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6008 - loss: 0.6730 - val_accuracy: 0.5587 - val_loss: 0.6832\n",
      "Epoch 11/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5550 - loss: 0.6851 - val_accuracy: 0.5669 - val_loss: 0.6786\n",
      "Epoch 12/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5923 - loss: 0.6734 - val_accuracy: 0.5763 - val_loss: 0.6798\n",
      "Epoch 13/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5878 - loss: 0.6714 - val_accuracy: 0.5706 - val_loss: 0.6762\n",
      "Epoch 14/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6045 - loss: 0.6621 - val_accuracy: 0.5806 - val_loss: 0.6775\n",
      "Epoch 15/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5835 - loss: 0.6709 - val_accuracy: 0.5788 - val_loss: 0.6741\n",
      "Epoch 16/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6037 - loss: 0.6653 - val_accuracy: 0.5906 - val_loss: 0.6725\n",
      "Epoch 17/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6194 - loss: 0.6521 - val_accuracy: 0.5844 - val_loss: 0.6762\n",
      "Epoch 18/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5850 - loss: 0.6715 - val_accuracy: 0.5900 - val_loss: 0.6698\n",
      "Epoch 19/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5843 - loss: 0.6702 - val_accuracy: 0.5919 - val_loss: 0.6798\n",
      "Fold 3 - Pérdida de validación: 0.66982\n",
      "Fold 3 - Accuracy de validación: 0.59000\n",
      "Fold 4: \n",
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.5166 - loss: 0.7020 - val_accuracy: 0.4969 - val_loss: 0.6908\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5201 - loss: 0.6947 - val_accuracy: 0.4794 - val_loss: 0.6991\n",
      "Epoch 3/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5238 - loss: 0.6968 - val_accuracy: 0.5206 - val_loss: 0.6899\n",
      "Epoch 4/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5021 - loss: 0.6957 - val_accuracy: 0.6106 - val_loss: 0.6805\n",
      "Epoch 5/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5427 - loss: 0.6878 - val_accuracy: 0.4844 - val_loss: 0.6933\n",
      "Epoch 6/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5345 - loss: 0.6890 - val_accuracy: 0.5612 - val_loss: 0.6841\n",
      "Epoch 7/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5307 - loss: 0.6937 - val_accuracy: 0.5256 - val_loss: 0.6877\n",
      "Epoch 8/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5443 - loss: 0.6876 - val_accuracy: 0.5781 - val_loss: 0.6750\n",
      "Epoch 9/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5809 - loss: 0.6811 - val_accuracy: 0.6031 - val_loss: 0.6714\n",
      "Epoch 10/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5814 - loss: 0.6777 - val_accuracy: 0.6194 - val_loss: 0.6630\n",
      "Epoch 11/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5848 - loss: 0.6751 - val_accuracy: 0.5863 - val_loss: 0.6730\n",
      "Epoch 12/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5619 - loss: 0.6792 - val_accuracy: 0.6219 - val_loss: 0.6640\n",
      "Epoch 13/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5753 - loss: 0.6754 - val_accuracy: 0.4888 - val_loss: 0.7002\n",
      "Epoch 14/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5618 - loss: 0.6802 - val_accuracy: 0.5906 - val_loss: 0.6743\n",
      "Epoch 15/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6036 - loss: 0.6724 - val_accuracy: 0.5694 - val_loss: 0.6763\n",
      "Epoch 16/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5790 - loss: 0.6764 - val_accuracy: 0.5931 - val_loss: 0.6679\n",
      "Epoch 17/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5896 - loss: 0.6723 - val_accuracy: 0.5731 - val_loss: 0.6701\n",
      "Epoch 18/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5868 - loss: 0.6712 - val_accuracy: 0.6356 - val_loss: 0.6497\n",
      "Epoch 19/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6108 - loss: 0.6620 - val_accuracy: 0.6256 - val_loss: 0.6587\n",
      "Epoch 20/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5925 - loss: 0.6659 - val_accuracy: 0.6294 - val_loss: 0.6606\n",
      "Epoch 21/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5940 - loss: 0.6668 - val_accuracy: 0.6112 - val_loss: 0.6573\n",
      "Epoch 22/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5893 - loss: 0.6726 - val_accuracy: 0.6231 - val_loss: 0.6561\n",
      "Epoch 23/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5922 - loss: 0.6647 - val_accuracy: 0.6344 - val_loss: 0.6525\n",
      "Epoch 24/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5992 - loss: 0.6624 - val_accuracy: 0.6500 - val_loss: 0.6496\n",
      "Epoch 25/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5607 - loss: 0.6776 - val_accuracy: 0.6244 - val_loss: 0.6694\n",
      "Epoch 26/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6048 - loss: 0.6717 - val_accuracy: 0.6375 - val_loss: 0.6512\n",
      "Epoch 27/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5949 - loss: 0.6692 - val_accuracy: 0.5719 - val_loss: 0.6786\n",
      "Epoch 28/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5936 - loss: 0.6685 - val_accuracy: 0.6438 - val_loss: 0.6495\n",
      "Fold 4 - Pérdida de validación: 0.64946\n",
      "Fold 4 - Accuracy de validación: 0.64375\n",
      "Fold 5: \n",
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.5025 - loss: 0.7120 - val_accuracy: 0.4981 - val_loss: 0.6924\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5280 - loss: 0.6960 - val_accuracy: 0.5025 - val_loss: 0.6924\n",
      "Epoch 3/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4967 - loss: 0.6969 - val_accuracy: 0.5638 - val_loss: 0.6905\n",
      "Epoch 4/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5266 - loss: 0.6902 - val_accuracy: 0.5594 - val_loss: 0.6908\n",
      "Epoch 5/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5366 - loss: 0.6899 - val_accuracy: 0.5731 - val_loss: 0.6860\n",
      "Epoch 6/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5501 - loss: 0.6872 - val_accuracy: 0.5725 - val_loss: 0.6826\n",
      "Epoch 7/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.5615 - loss: 0.6833 - val_accuracy: 0.5831 - val_loss: 0.6800\n",
      "Epoch 8/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5651 - loss: 0.6821 - val_accuracy: 0.5700 - val_loss: 0.6796\n",
      "Epoch 9/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5983 - loss: 0.6728 - val_accuracy: 0.5569 - val_loss: 0.6841\n",
      "Epoch 10/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5929 - loss: 0.6753 - val_accuracy: 0.5875 - val_loss: 0.6764\n",
      "Epoch 11/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5703 - loss: 0.6776 - val_accuracy: 0.5875 - val_loss: 0.6754\n",
      "Epoch 12/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5782 - loss: 0.6751 - val_accuracy: 0.5813 - val_loss: 0.6800\n",
      "Epoch 13/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5792 - loss: 0.6749 - val_accuracy: 0.5931 - val_loss: 0.6755\n",
      "Epoch 14/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5956 - loss: 0.6750 - val_accuracy: 0.5919 - val_loss: 0.6752\n",
      "Epoch 15/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5678 - loss: 0.6804 - val_accuracy: 0.5975 - val_loss: 0.6718\n",
      "Epoch 16/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5942 - loss: 0.6725 - val_accuracy: 0.5987 - val_loss: 0.6693\n",
      "Epoch 17/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5892 - loss: 0.6693 - val_accuracy: 0.5800 - val_loss: 0.6803\n",
      "Epoch 18/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5793 - loss: 0.6735 - val_accuracy: 0.5938 - val_loss: 0.6800\n",
      "Epoch 19/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5776 - loss: 0.6758 - val_accuracy: 0.5369 - val_loss: 0.6896\n",
      "Epoch 20/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5976 - loss: 0.6666 - val_accuracy: 0.5888 - val_loss: 0.6710\n",
      "Epoch 21/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5861 - loss: 0.6693 - val_accuracy: 0.5731 - val_loss: 0.6802\n",
      "Epoch 22/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5697 - loss: 0.6772 - val_accuracy: 0.5919 - val_loss: 0.6709\n",
      "Epoch 23/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6122 - loss: 0.6619 - val_accuracy: 0.5900 - val_loss: 0.6700\n",
      "Epoch 24/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6023 - loss: 0.6674 - val_accuracy: 0.5419 - val_loss: 0.6878\n",
      "Epoch 25/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6003 - loss: 0.6628 - val_accuracy: 0.5975 - val_loss: 0.6729\n",
      "Epoch 26/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6045 - loss: 0.6693 - val_accuracy: 0.5644 - val_loss: 0.6804\n",
      "Fold 5 - Pérdida de validación: 0.66932\n",
      "Fold 5 - Accuracy de validación: 0.59875\n",
      "Parámetros: (64, 0.1, 'relu'),                 Porcentaje: 0.5,                 Tiempo: 00:00:11,                 Mean Accuracy: 0.6026,                 Mean Loss: 0.6690\n",
      "Parámetros: (64, 0.2, 'relu'),                 Porcentaje: 0.5,                 Tiempo: 00:00:07,                 Mean Accuracy: 0.5274,                 Mean Loss: 0.6904\n",
      "Parámetros: (64, 0.3, 'relu'),                 Porcentaje: 0.5,                 Tiempo: 00:00:06,                 Mean Accuracy: 0.5370,                 Mean Loss: 0.6922\n",
      "Parámetros: (128, 0.1, 'relu'),                 Porcentaje: 0.5,                 Tiempo: 00:00:14,                 Mean Accuracy: 0.6036,                 Mean Loss: 0.6663\n",
      "Parámetros: (128, 0.2, 'relu'),                 Porcentaje: 0.5,                 Tiempo: 00:00:08,                 Mean Accuracy: 0.5504,                 Mean Loss: 0.6837\n",
      "Parámetros: (128, 0.3, 'relu'),                 Porcentaje: 0.5,                 Tiempo: 00:00:07,                 Mean Accuracy: 0.5536,                 Mean Loss: 0.6912\n",
      "Parámetros: (256, 0.1, 'relu'),                 Porcentaje: 0.5,                 Tiempo: 00:00:13,                 Mean Accuracy: 0.6030,                 Mean Loss: 0.6678\n",
      "\n",
      "Probando combinación de parámetros: (256, 0.2, 'relu')\n",
      "Fold 1: \n",
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.4922 - loss: 0.7359 - val_accuracy: 0.5156 - val_loss: 0.6901\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5028 - loss: 0.7035 - val_accuracy: 0.5150 - val_loss: 0.6916\n",
      "Epoch 3/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5061 - loss: 0.6970 - val_accuracy: 0.5219 - val_loss: 0.6917\n",
      "Epoch 4/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5269 - loss: 0.6947 - val_accuracy: 0.5138 - val_loss: 0.6924\n",
      "Epoch 5/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5071 - loss: 0.6942 - val_accuracy: 0.5144 - val_loss: 0.6912\n",
      "Epoch 6/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5113 - loss: 0.6943 - val_accuracy: 0.4863 - val_loss: 0.6927\n",
      "Epoch 7/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5148 - loss: 0.6936 - val_accuracy: 0.4875 - val_loss: 0.6927\n",
      "Epoch 8/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5202 - loss: 0.6934 - val_accuracy: 0.4863 - val_loss: 0.6956\n",
      "Epoch 9/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5125 - loss: 0.6927 - val_accuracy: 0.5238 - val_loss: 0.6887\n",
      "Epoch 10/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5426 - loss: 0.6881 - val_accuracy: 0.4863 - val_loss: 0.6961\n",
      "Epoch 11/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5218 - loss: 0.6907 - val_accuracy: 0.5625 - val_loss: 0.6906\n",
      "Fold 1 - Pérdida de validación: 0.68873\n",
      "Fold 1 - Accuracy de validación: 0.52375\n",
      "Fold 2: \n",
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.4853 - loss: 0.7235 - val_accuracy: 0.5244 - val_loss: 0.6929\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.4941 - loss: 0.7012 - val_accuracy: 0.4863 - val_loss: 0.6944\n",
      "Epoch 3/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5051 - loss: 0.6959 - val_accuracy: 0.4863 - val_loss: 0.6959\n",
      "Epoch 4/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5005 - loss: 0.6976 - val_accuracy: 0.4863 - val_loss: 0.6938\n",
      "Epoch 5/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.4904 - loss: 0.6939 - val_accuracy: 0.5138 - val_loss: 0.6924\n",
      "Epoch 6/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5024 - loss: 0.6932 - val_accuracy: 0.4863 - val_loss: 0.6931\n",
      "Epoch 7/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.4839 - loss: 0.6957 - val_accuracy: 0.4863 - val_loss: 0.6934\n",
      "Epoch 8/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5089 - loss: 0.6927 - val_accuracy: 0.5031 - val_loss: 0.6912\n",
      "Epoch 9/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5144 - loss: 0.6920 - val_accuracy: 0.5925 - val_loss: 0.6874\n",
      "Epoch 10/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5253 - loss: 0.6928 - val_accuracy: 0.4863 - val_loss: 0.6999\n",
      "Epoch 11/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4954 - loss: 0.6964 - val_accuracy: 0.5194 - val_loss: 0.6925\n",
      "Fold 2 - Pérdida de validación: 0.68739\n",
      "Fold 2 - Accuracy de validación: 0.59250\n",
      "Fold 3: \n",
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.4924 - loss: 0.7349 - val_accuracy: 0.5088 - val_loss: 0.6929\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.4974 - loss: 0.6998 - val_accuracy: 0.5081 - val_loss: 0.6921\n",
      "Epoch 3/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5004 - loss: 0.6956 - val_accuracy: 0.5688 - val_loss: 0.6923\n",
      "Epoch 4/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5231 - loss: 0.6922 - val_accuracy: 0.5575 - val_loss: 0.6898\n",
      "Epoch 5/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5110 - loss: 0.6930 - val_accuracy: 0.5094 - val_loss: 0.6908\n",
      "Epoch 6/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5167 - loss: 0.6925 - val_accuracy: 0.4975 - val_loss: 0.6922\n",
      "Epoch 7/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5284 - loss: 0.6917 - val_accuracy: 0.5544 - val_loss: 0.6896\n",
      "Epoch 8/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5511 - loss: 0.6865 - val_accuracy: 0.5231 - val_loss: 0.6940\n",
      "Epoch 9/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5511 - loss: 0.6845 - val_accuracy: 0.5706 - val_loss: 0.6843\n",
      "Epoch 10/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5550 - loss: 0.6796 - val_accuracy: 0.5225 - val_loss: 0.7045\n",
      "Epoch 11/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5358 - loss: 0.6889 - val_accuracy: 0.5113 - val_loss: 0.6923\n",
      "Fold 3 - Pérdida de validación: 0.68430\n",
      "Fold 3 - Accuracy de validación: 0.57063\n",
      "Fold 4: \n",
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.5235 - loss: 0.7190 - val_accuracy: 0.5681 - val_loss: 0.6911\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.4980 - loss: 0.7011 - val_accuracy: 0.4794 - val_loss: 0.6940\n",
      "Epoch 3/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4981 - loss: 0.6989 - val_accuracy: 0.4888 - val_loss: 0.6930\n",
      "Epoch 4/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5213 - loss: 0.6917 - val_accuracy: 0.4837 - val_loss: 0.6926\n",
      "Epoch 5/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5353 - loss: 0.6932 - val_accuracy: 0.5163 - val_loss: 0.6909\n",
      "Epoch 6/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5140 - loss: 0.6917 - val_accuracy: 0.5663 - val_loss: 0.6899\n",
      "Epoch 7/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5261 - loss: 0.6904 - val_accuracy: 0.6075 - val_loss: 0.6846\n",
      "Epoch 8/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5444 - loss: 0.6894 - val_accuracy: 0.5888 - val_loss: 0.6879\n",
      "Epoch 9/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5560 - loss: 0.6883 - val_accuracy: 0.6037 - val_loss: 0.6823\n",
      "Epoch 10/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5379 - loss: 0.6863 - val_accuracy: 0.6200 - val_loss: 0.6790\n",
      "Epoch 11/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5447 - loss: 0.6878 - val_accuracy: 0.5475 - val_loss: 0.6866\n",
      "Epoch 12/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5453 - loss: 0.6895 - val_accuracy: 0.6062 - val_loss: 0.6822\n",
      "Epoch 13/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5501 - loss: 0.6847 - val_accuracy: 0.6156 - val_loss: 0.6689\n",
      "Epoch 14/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5703 - loss: 0.6807 - val_accuracy: 0.5337 - val_loss: 0.6815\n",
      "Epoch 15/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5429 - loss: 0.6878 - val_accuracy: 0.6031 - val_loss: 0.6834\n",
      "Epoch 16/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5725 - loss: 0.6821 - val_accuracy: 0.5175 - val_loss: 0.6933\n",
      "Epoch 17/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5331 - loss: 0.6897 - val_accuracy: 0.6181 - val_loss: 0.6751\n",
      "Epoch 18/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5529 - loss: 0.6866 - val_accuracy: 0.5619 - val_loss: 0.6852\n",
      "Epoch 19/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5520 - loss: 0.6866 - val_accuracy: 0.6144 - val_loss: 0.6741\n",
      "Epoch 20/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5860 - loss: 0.6764 - val_accuracy: 0.6363 - val_loss: 0.6678\n",
      "Epoch 21/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5723 - loss: 0.6821 - val_accuracy: 0.6200 - val_loss: 0.6700\n",
      "Epoch 22/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5710 - loss: 0.6801 - val_accuracy: 0.6338 - val_loss: 0.6690\n",
      "Epoch 23/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5877 - loss: 0.6776 - val_accuracy: 0.6419 - val_loss: 0.6640\n",
      "Fold 4 - Pérdida de validación: 0.66395\n",
      "Fold 4 - Accuracy de validación: 0.64188\n",
      "Fold 5: \n",
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.4925 - loss: 0.7381 - val_accuracy: 0.5194 - val_loss: 0.6930\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5098 - loss: 0.6969 - val_accuracy: 0.4975 - val_loss: 0.6956\n",
      "Epoch 3/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5090 - loss: 0.6963 - val_accuracy: 0.4975 - val_loss: 0.6928\n",
      "Epoch 4/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5138 - loss: 0.6939 - val_accuracy: 0.5312 - val_loss: 0.6917\n",
      "Epoch 5/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4934 - loss: 0.6967 - val_accuracy: 0.4919 - val_loss: 0.6931\n",
      "Epoch 6/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5176 - loss: 0.6940 - val_accuracy: 0.4938 - val_loss: 0.6919\n",
      "Epoch 7/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5177 - loss: 0.6908 - val_accuracy: 0.5156 - val_loss: 0.6893\n",
      "Epoch 8/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5324 - loss: 0.6906 - val_accuracy: 0.5769 - val_loss: 0.6859\n",
      "Epoch 9/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5663 - loss: 0.6860 - val_accuracy: 0.5081 - val_loss: 0.6909\n",
      "Epoch 10/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5454 - loss: 0.6862 - val_accuracy: 0.4913 - val_loss: 0.6927\n",
      "Epoch 11/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5418 - loss: 0.6860 - val_accuracy: 0.5644 - val_loss: 0.6809\n",
      "Epoch 12/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5675 - loss: 0.6824 - val_accuracy: 0.5769 - val_loss: 0.6807\n",
      "Epoch 13/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5874 - loss: 0.6782 - val_accuracy: 0.5688 - val_loss: 0.6811\n",
      "Epoch 14/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5643 - loss: 0.6815 - val_accuracy: 0.5819 - val_loss: 0.6769\n",
      "Epoch 15/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5694 - loss: 0.6791 - val_accuracy: 0.5881 - val_loss: 0.6774\n",
      "Epoch 16/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5741 - loss: 0.6772 - val_accuracy: 0.5906 - val_loss: 0.6756\n",
      "Epoch 17/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5854 - loss: 0.6730 - val_accuracy: 0.5750 - val_loss: 0.6779\n",
      "Epoch 18/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5934 - loss: 0.6657 - val_accuracy: 0.5081 - val_loss: 0.7001\n",
      "Epoch 19/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5023 - loss: 0.6995 - val_accuracy: 0.5894 - val_loss: 0.6783\n",
      "Epoch 20/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5820 - loss: 0.6749 - val_accuracy: 0.5706 - val_loss: 0.6806\n",
      "Epoch 21/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5861 - loss: 0.6712 - val_accuracy: 0.5788 - val_loss: 0.6777\n",
      "Fold 5 - Pérdida de validación: 0.67560\n",
      "Fold 5 - Accuracy de validación: 0.59062\n",
      "Parámetros: (64, 0.1, 'relu'),                 Porcentaje: 0.5,                 Tiempo: 00:00:11,                 Mean Accuracy: 0.6026,                 Mean Loss: 0.6690\n",
      "Parámetros: (64, 0.2, 'relu'),                 Porcentaje: 0.5,                 Tiempo: 00:00:07,                 Mean Accuracy: 0.5274,                 Mean Loss: 0.6904\n",
      "Parámetros: (64, 0.3, 'relu'),                 Porcentaje: 0.5,                 Tiempo: 00:00:06,                 Mean Accuracy: 0.5370,                 Mean Loss: 0.6922\n",
      "Parámetros: (128, 0.1, 'relu'),                 Porcentaje: 0.5,                 Tiempo: 00:00:14,                 Mean Accuracy: 0.6036,                 Mean Loss: 0.6663\n",
      "Parámetros: (128, 0.2, 'relu'),                 Porcentaje: 0.5,                 Tiempo: 00:00:08,                 Mean Accuracy: 0.5504,                 Mean Loss: 0.6837\n",
      "Parámetros: (128, 0.3, 'relu'),                 Porcentaje: 0.5,                 Tiempo: 00:00:07,                 Mean Accuracy: 0.5536,                 Mean Loss: 0.6912\n",
      "Parámetros: (256, 0.1, 'relu'),                 Porcentaje: 0.5,                 Tiempo: 00:00:13,                 Mean Accuracy: 0.6030,                 Mean Loss: 0.6678\n",
      "Parámetros: (256, 0.2, 'relu'),                 Porcentaje: 0.5,                 Tiempo: 00:00:10,                 Mean Accuracy: 0.5839,                 Mean Loss: 0.6800\n",
      "\n",
      "Probando combinación de parámetros: (256, 0.3, 'relu')\n",
      "Fold 1: \n",
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.4877 - loss: 0.7443 - val_accuracy: 0.5150 - val_loss: 0.6919\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5112 - loss: 0.6993 - val_accuracy: 0.4863 - val_loss: 0.6959\n",
      "Epoch 3/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.4922 - loss: 0.6973 - val_accuracy: 0.4856 - val_loss: 0.6930\n",
      "Epoch 4/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5054 - loss: 0.6931 - val_accuracy: 0.5206 - val_loss: 0.6923\n",
      "Epoch 5/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5139 - loss: 0.6935 - val_accuracy: 0.5200 - val_loss: 0.6927\n",
      "Epoch 6/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4932 - loss: 0.6937 - val_accuracy: 0.4863 - val_loss: 0.6932\n",
      "Epoch 7/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5010 - loss: 0.6931 - val_accuracy: 0.4863 - val_loss: 0.6934\n",
      "Epoch 8/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.4858 - loss: 0.6933 - val_accuracy: 0.4863 - val_loss: 0.6932\n",
      "Epoch 9/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4960 - loss: 0.6933 - val_accuracy: 0.5138 - val_loss: 0.6931\n",
      "Epoch 10/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5112 - loss: 0.6931 - val_accuracy: 0.5138 - val_loss: 0.6931\n",
      "Epoch 11/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5002 - loss: 0.6930 - val_accuracy: 0.4863 - val_loss: 0.6932\n",
      "Fold 1 - Pérdida de validación: 0.69186\n",
      "Fold 1 - Accuracy de validación: 0.51500\n",
      "Fold 2: \n",
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.4945 - loss: 0.7503 - val_accuracy: 0.4931 - val_loss: 0.6927\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5011 - loss: 0.7025 - val_accuracy: 0.5138 - val_loss: 0.6923\n",
      "Epoch 3/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5092 - loss: 0.6999 - val_accuracy: 0.5113 - val_loss: 0.6929\n",
      "Epoch 4/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4797 - loss: 0.6977 - val_accuracy: 0.5688 - val_loss: 0.6924\n",
      "Epoch 5/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5042 - loss: 0.6941 - val_accuracy: 0.5144 - val_loss: 0.6918\n",
      "Epoch 6/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5137 - loss: 0.6936 - val_accuracy: 0.5138 - val_loss: 0.6922\n",
      "Epoch 7/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5045 - loss: 0.6939 - val_accuracy: 0.4750 - val_loss: 0.6928\n",
      "Epoch 8/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5210 - loss: 0.6929 - val_accuracy: 0.5319 - val_loss: 0.6901\n",
      "Epoch 9/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5380 - loss: 0.6905 - val_accuracy: 0.4863 - val_loss: 0.6932\n",
      "Epoch 10/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4939 - loss: 0.6932 - val_accuracy: 0.4863 - val_loss: 0.6932\n",
      "Epoch 11/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4848 - loss: 0.6933 - val_accuracy: 0.4863 - val_loss: 0.6933\n",
      "Fold 2 - Pérdida de validación: 0.69009\n",
      "Fold 2 - Accuracy de validación: 0.53188\n",
      "Fold 3: \n",
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.5016 - loss: 0.7314 - val_accuracy: 0.5038 - val_loss: 0.6921\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4999 - loss: 0.7098 - val_accuracy: 0.5088 - val_loss: 0.6931\n",
      "Epoch 3/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5133 - loss: 0.6971 - val_accuracy: 0.5225 - val_loss: 0.6916\n",
      "Epoch 4/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5225 - loss: 0.6951 - val_accuracy: 0.4931 - val_loss: 0.6932\n",
      "Epoch 5/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4920 - loss: 0.6963 - val_accuracy: 0.5088 - val_loss: 0.6928\n",
      "Epoch 6/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4960 - loss: 0.6936 - val_accuracy: 0.5088 - val_loss: 0.6922\n",
      "Epoch 7/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5137 - loss: 0.6925 - val_accuracy: 0.4919 - val_loss: 0.6955\n",
      "Epoch 8/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5283 - loss: 0.6919 - val_accuracy: 0.5456 - val_loss: 0.6925\n",
      "Epoch 9/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5436 - loss: 0.6898 - val_accuracy: 0.4950 - val_loss: 0.6941\n",
      "Epoch 10/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5121 - loss: 0.6930 - val_accuracy: 0.5537 - val_loss: 0.6904\n",
      "Epoch 11/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5222 - loss: 0.6904 - val_accuracy: 0.4906 - val_loss: 0.6935\n",
      "Fold 3 - Pérdida de validación: 0.69044\n",
      "Fold 3 - Accuracy de validación: 0.55375\n",
      "Fold 4: \n",
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.4811 - loss: 0.7268 - val_accuracy: 0.4794 - val_loss: 0.6950\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5140 - loss: 0.6995 - val_accuracy: 0.4794 - val_loss: 0.6967\n",
      "Epoch 3/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5180 - loss: 0.6946 - val_accuracy: 0.4825 - val_loss: 0.6927\n",
      "Epoch 4/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5055 - loss: 0.6957 - val_accuracy: 0.4794 - val_loss: 0.6959\n",
      "Epoch 5/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5018 - loss: 0.6948 - val_accuracy: 0.5888 - val_loss: 0.6924\n",
      "Epoch 6/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4975 - loss: 0.6923 - val_accuracy: 0.4844 - val_loss: 0.6925\n",
      "Epoch 7/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4980 - loss: 0.6914 - val_accuracy: 0.5206 - val_loss: 0.6930\n",
      "Epoch 8/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4954 - loss: 0.6932 - val_accuracy: 0.4794 - val_loss: 0.6934\n",
      "Epoch 9/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5013 - loss: 0.6931 - val_accuracy: 0.4794 - val_loss: 0.6934\n",
      "Epoch 10/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4988 - loss: 0.6932 - val_accuracy: 0.4794 - val_loss: 0.6935\n",
      "Epoch 11/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5144 - loss: 0.6930 - val_accuracy: 0.4794 - val_loss: 0.6937\n",
      "Fold 4 - Pérdida de validación: 0.69244\n",
      "Fold 4 - Accuracy de validación: 0.58875\n",
      "Fold 5: \n",
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.5040 - loss: 0.7643 - val_accuracy: 0.5163 - val_loss: 0.6924\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5076 - loss: 0.7011 - val_accuracy: 0.5487 - val_loss: 0.6927\n",
      "Epoch 3/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5003 - loss: 0.6993 - val_accuracy: 0.4969 - val_loss: 0.6914\n",
      "Epoch 4/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5283 - loss: 0.6968 - val_accuracy: 0.5025 - val_loss: 0.6928\n",
      "Epoch 5/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5119 - loss: 0.6944 - val_accuracy: 0.5031 - val_loss: 0.6930\n",
      "Epoch 6/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5300 - loss: 0.6930 - val_accuracy: 0.5213 - val_loss: 0.6907\n",
      "Epoch 7/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5277 - loss: 0.6899 - val_accuracy: 0.4981 - val_loss: 0.6890\n",
      "Epoch 8/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4862 - loss: 0.6935 - val_accuracy: 0.4975 - val_loss: 0.6932\n",
      "Epoch 9/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5159 - loss: 0.6929 - val_accuracy: 0.5369 - val_loss: 0.6922\n",
      "Epoch 10/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5072 - loss: 0.6928 - val_accuracy: 0.5138 - val_loss: 0.6929\n",
      "Epoch 11/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4914 - loss: 0.6932 - val_accuracy: 0.5119 - val_loss: 0.6930\n",
      "Fold 5 - Pérdida de validación: 0.68902\n",
      "Fold 5 - Accuracy de validación: 0.49812\n",
      "Parámetros: (64, 0.1, 'relu'),                 Porcentaje: 0.5,                 Tiempo: 00:00:11,                 Mean Accuracy: 0.6026,                 Mean Loss: 0.6690\n",
      "Parámetros: (64, 0.2, 'relu'),                 Porcentaje: 0.5,                 Tiempo: 00:00:07,                 Mean Accuracy: 0.5274,                 Mean Loss: 0.6904\n",
      "Parámetros: (64, 0.3, 'relu'),                 Porcentaje: 0.5,                 Tiempo: 00:00:06,                 Mean Accuracy: 0.5370,                 Mean Loss: 0.6922\n",
      "Parámetros: (128, 0.1, 'relu'),                 Porcentaje: 0.5,                 Tiempo: 00:00:14,                 Mean Accuracy: 0.6036,                 Mean Loss: 0.6663\n",
      "Parámetros: (128, 0.2, 'relu'),                 Porcentaje: 0.5,                 Tiempo: 00:00:08,                 Mean Accuracy: 0.5504,                 Mean Loss: 0.6837\n",
      "Parámetros: (128, 0.3, 'relu'),                 Porcentaje: 0.5,                 Tiempo: 00:00:07,                 Mean Accuracy: 0.5536,                 Mean Loss: 0.6912\n",
      "Parámetros: (256, 0.1, 'relu'),                 Porcentaje: 0.5,                 Tiempo: 00:00:13,                 Mean Accuracy: 0.6030,                 Mean Loss: 0.6678\n",
      "Parámetros: (256, 0.2, 'relu'),                 Porcentaje: 0.5,                 Tiempo: 00:00:10,                 Mean Accuracy: 0.5839,                 Mean Loss: 0.6800\n",
      "Parámetros: (256, 0.3, 'relu'),                 Porcentaje: 0.5,                 Tiempo: 00:00:07,                 Mean Accuracy: 0.5375,                 Mean Loss: 0.6908\n"
     ]
    }
   ],
   "source": [
    "percent = 0.5\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "for params in param_combinations:\n",
    "    units, dropout_rate, activation = params\n",
    "    print(f\"\\nProbando combinación de parámetros: {params}\")\n",
    "\n",
    "    accuracies = []\n",
    "    losses = []\n",
    "    histories = []\n",
    "    times = []\n",
    "    fold = 1\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        print(f\"Fold {fold}: \")\n",
    "\n",
    "        xtrn, xval = X_train[train_index], X_train[val_index]\n",
    "        ytrn, yval = y_train[train_index], y_train[val_index] \n",
    "\n",
    "        xtrn = xtrn[: int(len(xtrn) * percent)]\n",
    "        ytrn = ytrn[: int(len(ytrn) * percent)]\n",
    "\n",
    "        # Crear el EarlyStopping callback\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor=\"val_loss\", patience=10, min_delta=0.01, restore_best_weights=True\n",
    "        )\n",
    "\n",
    "        # Crear el modelo con los parámetros actuales\n",
    "        model = TRAINING[MODEL](\n",
    "            optimizer=\"adam\",\n",
    "            units=units,\n",
    "            activation=activation,\n",
    "            dropout_rate=dropout_rate,\n",
    "        )\n",
    "\n",
    "        # Entrenar el modelo\n",
    "        start_time = time.time()\n",
    "        history = model.fit(\n",
    "            xtrn,\n",
    "            ytrn,\n",
    "            validation_data=(xval, yval),\n",
    "            epochs=200,\n",
    "            batch_size=64,\n",
    "            verbose=1,\n",
    "            callbacks=[early_stopping],\n",
    "        )\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Obtener la mejor epoca\n",
    "        idx = np.argmin(history.history[\"val_loss\"])\n",
    "        losses.append(history.history[\"val_loss\"][idx])\n",
    "        accuracies.append(history.history[\"val_accuracy\"][idx])\n",
    "        times.append(end_time - start_time)\n",
    "        print(f\"Fold {fold} - Pérdida de validación: {losses[-1]:.5f}\")\n",
    "        print(f\"Fold {fold} - Accuracy de validación: {accuracies[-1]:.5f}\")\n",
    "\n",
    "        histories.append(history)\n",
    "\n",
    "        del model\n",
    "        fold += 1\n",
    "\n",
    "        # Liberar recursos de memoria\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "\n",
    "    # Almacenar métricas de la combinación actual\n",
    "    mean_time = np.mean(times)\n",
    "    time_str = time.strftime(\"%H:%M:%S\", time.gmtime(mean_time))\n",
    "    results.append(\n",
    "        {\n",
    "            \"params\": params,\n",
    "            \"percent\": percent,\n",
    "            \"time\": time_str,\n",
    "            \"mean_accuracy\": np.mean(accuracies),\n",
    "            \"mean_loss\": np.mean(losses),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Imprimir los resultados finales\n",
    "    for result in results:\n",
    "        print(\n",
    "            f\"Parámetros: {result['params']}, \\\n",
    "                Porcentaje: {result['percent']}, \\\n",
    "                Tiempo: {result['time']}, \\\n",
    "                Mean Accuracy: {result['mean_accuracy']:.4f}, \\\n",
    "                Mean Loss: {result['mean_loss']:.4f}\"\n",
    "        )\n",
    "\n",
    "    # Store model with timestamp with year, month, day, hour, minute and second\n",
    "    timestamp = time.strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "    # Save params in a .txt file for every model\n",
    "    with open(f\"../../params/{MODEL}_{FEATURE}.txt\", \"a\") as f:\n",
    "        f.write(f\"ID: {timestamp}\\n\")\n",
    "        f.write(f\"Porcentaje: {result['percent']}\\n\")\n",
    "        f.write(f\"Parámetros: {result['params']}\\n\")\n",
    "        f.write(f\"Tiempo medio: {result['time']}\\n\")\n",
    "        f.write(f\"Accuracy medio: {result['mean_accuracy']:.4f}\\n\")\n",
    "        f.write(f\"Loss medio: {result['mean_loss']:.4f}\\n\\n\")\n",
    "\n",
    "    # Plot acc and loss for each fold, training and validation\n",
    "    # Using a k, 2 subplot grid\n",
    "    plt.figure(figsize=(15, 6))\n",
    "\n",
    "    for i in range(k):\n",
    "        plt.subplot(2, k, i + 1)\n",
    "        plt.plot(histories[i].history[\"loss\"], label=f\"Fold {i + 1} Training\")\n",
    "        plt.plot(histories[i].history[\"val_loss\"], label=f\"Fold {i + 1} Validation\")\n",
    "        plt.title(\"Loss\")\n",
    "        plt.ylim(0, 0.8)\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(2, k, i + 1 + k)\n",
    "        plt.plot(histories[i].history[\"accuracy\"], label=f\"Fold {i + 1} Training\")\n",
    "        plt.plot(histories[i].history[\"val_accuracy\"], label=f\"Fold {i + 1} Validation\")\n",
    "        plt.title(\"Accuracy\")\n",
    "        plt.ylim(0.5, 1)\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_fig(MODEL, FEATURE, timestamp)\n",
    "    plt.close()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.5072 - loss: 0.7028\n",
      "Epoch 2/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5021 - loss: 0.6959\n",
      "Epoch 3/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5361 - loss: 0.6893\n",
      "Epoch 4/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5660 - loss: 0.6825\n",
      "Epoch 5/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5697 - loss: 0.6810\n",
      "Epoch 6/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5819 - loss: 0.6745\n",
      "Epoch 7/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5889 - loss: 0.6741\n",
      "Epoch 8/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5784 - loss: 0.6789\n",
      "Epoch 9/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5993 - loss: 0.6699\n",
      "Epoch 10/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5935 - loss: 0.6726\n",
      "Epoch 11/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6044 - loss: 0.6675\n",
      "Epoch 12/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5450 - loss: 0.6885\n",
      "Epoch 13/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5475 - loss: 0.6878\n",
      "Epoch 14/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5851 - loss: 0.6755\n",
      "Epoch 15/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5922 - loss: 0.6753\n",
      "Epoch 16/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5861 - loss: 0.6768\n",
      "Epoch 17/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5888 - loss: 0.6743\n",
      "Epoch 18/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5803 - loss: 0.6777\n",
      "Epoch 19/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5847 - loss: 0.6750\n",
      "Epoch 20/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6020 - loss: 0.6674\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5895 - loss: 0.6692\n",
      "Test Accuracy: 0.5960, Test Loss: 0.6673\n"
     ]
    }
   ],
   "source": [
    "# Test with best params\n",
    "units = 128\n",
    "dropout_rate = 0.1\n",
    "activation = \"relu\"\n",
    "\n",
    "# Crear el EarlyStopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"loss\", patience=25, min_delta=0.005, restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Crear el modelo con los parámetros actuales\n",
    "model = TRAINING[MODEL](\n",
    "    optimizer=\"adam\", units=units, activation=activation, dropout_rate=dropout_rate\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=64)\n",
    "\n",
    "# Evaluar el modelo\n",
    "loss, acc = model.evaluate(X_tst, y_tst)\n",
    "print(f\"Test Accuracy: {acc:.4f}, Test Loss: {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(f\"{MODELS_DIR}/{MODEL}_{FEATURE}_{loss:.4f}.keras\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
